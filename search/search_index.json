{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to bayes_traj documentation!","text":"<p>bayes_traj is a Python package developed to perform Bayesian trajectory analysis, and it has several features that distinguish it from other trajectory approaches:</p> <ul> <li>It is fully Bayesian, so it supports incorporation of prior knowledge into the modeling process.</li> <li>It can simultaneously model multiple continuous and binary target variables as a functions of predictor variables.</li> <li>It uses Bayesian nonparametrics to identify the number of trajectories best supported by the data.</li> <li>Inference is performed using coordinate ascent variational inference, so it is fast and scales well to large data sets.</li> <li>Independantly estimates residual variance for each trajectory and each target variable.</li> <li>Can we used without random effects (group-based trajectory modeling, or GBTM), or with random effects (growth mixture modeling, or GMM).</li> <li>Provides a suite of tools to facilitate prior specification, model visualization, and summary statistic computation. </li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To get started using bayes_traj, simply install it using pip by running the following in your terminal:</p> <pre><code>pip install bayes_traj\n</code></pre> <p>bayes_traj provides several command-line tools: </p> <ul> <li><code>generate_prior</code> -- used to speficy Bayesian priors for use the trajectory   modeling</li> <li><code>viz_data_prior_draws</code> -- provides visualization of random draws from the   prior</li> <li><code>bayes_traj_main</code> -- performs Bayesian trajectory modeling using a prior file</li> <li><code>viz_model_trajs</code> -- provides visualization of trajectories fit using   <code>bayes_traj_main</code></li> <li><code>sumarize_traj_model</code> -- prints model summary and fit statistics given a model   file produce by <code>bayes_traj_main</code></li> <li><code>assign_trajectory</code> -- writes a data file with appended trajectory assignment   information given an input data file and a model file generated by the   <code>bayes_traj_main</code> tool         </li> </ul> <p>Each of these tools can be run with the -h flag for additional usage information.</p>"},{"location":"#documentation-contents","title":"Documentation Contents","text":"<ul> <li>Tutorial -- provides a walk-through of basic usage</li> <li>Formulation -- states the Bayesian model assumptions and formulation</li> <li>Inference -- details the variational inference update equations</li> </ul>"},{"location":"bayes_traj_tutorial/","title":"Introduction","text":"<p>This tutorial demonstrates the basic procedure for performing Bayesian trajectory modeling using the bayes_traj python package. No knowledge of the python programming language is required.</p>"},{"location":"bayes_traj_tutorial/#data-preliminaries","title":"Data Preliminaries","text":""},{"location":"bayes_traj_tutorial/#general-comments-about-input-data","title":"General comments about input data","text":"<ul> <li>Tools expect data to be in csv (comma-separated value) format</li> <li>If an intercept term will be used as a predictor, the data set should contain a column of 1s</li> <li>There should be a subject identifier column (not strictly necessary, e.g. if your data is cross-sectional)</li> <li>bayes_traj uses '^' to indicate a predictor is being raised to a power and '*' to indicate an interaction between two predictors. </li> </ul>"},{"location":"bayes_traj_tutorial/#tutorial-data-description","title":"Tutorial data description","text":"<p>We begin by reading in a synthetically generated data set that mimics biomarkers (y1 and y2) that decline with age. There are five trajectory subgroups in our data set: three with modest rates of decline but with varying intercepts, and two with accelerated rates of decline (also with varying intercepts). There are three visits for each individual in our data set, simulating a longitudinal study. The visits are spread 5 years apart. Subject \"enrollment\" is 45 - 80 years old. </p> id intercept age age^2 y1 y2 0 1 1.0 68.2 4650.3 4.9 5.0 1 1 1.0 73.2 5357.2 5.0 4.9 2 1 1.0 78.2 6114.2 4.7 4.8 3 2 1.0 54.1 2931.0 5.3 5.2 4 2 1.0 59.1 3497.4 5.2 5.1 5 2 1.0 64.1 4113.7 5.0 5.1 6 3 1.0 58.3 3396.8 5.2 5.1 7 3 1.0 63.3 4004.7 5.0 5.0 8 3 1.0 68.3 4662.5 5.1 4.9 9 4 1.0 59.8 3575.5 5.1 5.0 <p>Let's now visualize the data:</p> <p></p> <p>This is a highly idealized data set. Working with such a data set initially has two purposes: 1) it makes clear what we are trying to achieve with trajectory analysis (namely, to identify subgroups and their characteristic progression patterns), and 2) it provides a \"sanity check\" that ensures the Bayesian trajectory tools produce the results we expect. We will explore more challenging data later in the tutorial.</p>"},{"location":"bayes_traj_tutorial/#generating-priors","title":"Generating Priors","text":"<p>Before we can perform Bayesian trajectory fitting to our data, we must first generate priors for the parameters in our model. The bayes_traj package provides two utilities for generating priors: generate_prior and viz_data_prior_draws. We can inspect tool usage by running each with the -h flag:</p> <pre><code>&gt; generate_prior -h\n</code></pre> <pre><code>usage: generate_prior [-h] [--preds PREDS] [--targets TARGETS]\n                      [--out_file OUT_FILE]\n                      [--tar_resid TAR_RESID [TAR_RESID ...]]\n                      [--coef COEF [COEF ...]]\n                      [--coef_std COEF_STD [COEF_STD ...]] [--in_data IN_DATA]\n                      [--ranefs RANEFS] [--ranef RANEF [RANEF ...]]\n                      [--est_ranefs] [--num_trajs NUM_TRAJS] [--model MODEL]\n                      [--model_trajs MODEL_TRAJS] [--groupby &lt;string&gt;]\n                      [--alpha &lt;class 'float'&gt;]\n\nGenerates a pickled file containing Bayesian trajectory prior information\n\noptions:\n  -h, --help            show this help message and exit\n  --preds PREDS         Comma-separated list of predictor names\n  --targets TARGETS     Comma-separated list of target names\n  --out_file OUT_FILE   Output (pickle) file that will contain the prior\n  --tar_resid TAR_RESID [TAR_RESID ...]\n                        Use this flag to specify the residual precision mean\n                        and variance for the corresponding target value.\n                        Specify as a comma-separated tuple:\n                        target_name,mean,var. Note that precision is the\n                        inverse of the variance. Only applies to continuous\n                        targets\n  --coef COEF [COEF ...]\n                        Coefficient prior for a specified target and\n                        predictor. Specify as a comma-separated tuple:\n                        target_name,predictor_name,mean,std\n  --coef_std COEF_STD [COEF_STD ...]\n                        Coefficient prior standard deviation for a specified\n                        target and predictor. Specify as a comma-separated\n                        tuple: target_name,predictor_name,std\n  --in_data IN_DATA     If a data file is specified, it will be read in and\n                        used to set reasonable prior values using regression.\n                        It is assumed that the file contains data columns with\n                        names corresponding to the predictor and target names\n                        specified on the command line.\n  --ranefs RANEFS       Comma-separated list of predictors for which to\n                        consider random effects (must be a subset of preds)\n  --ranef RANEF [RANEF ...]\n                        Variance or covariance prior for specified random\n                        effect. Specify as a comma-separated tuple:\n                        target_name,predictor_name,variance or\n                        target,predictor_name1, predictor_name2,covariance. By\n                        default, variances will be 1 and covariances will be\n                        0.\n  --est_ranefs          This flag will estimate the random effect covariance\n                        matrix from the input data or model. Note that this\n                        procedure may take several minutes. If random effects\n                        are not specified (using the --ranefs flag) this flag\n                        will have no effect. Note that by default, random\n                        effect covariance matrices will be set to the identify\n                        matrix.\n  --num_trajs NUM_TRAJS\n                        Estimate of the number of trajectories expected in the\n                        data set. Can be specified as a single value or as a\n                        dash-separated range, such as 4-6. If a single value\n                        is specified, a range will be assumed to be 1 to\n                        (2*num_trajs-1)\n  --model MODEL         Pickled bayes_traj model that has been fit to data and\n                        from which information will be extracted to produce an\n                        updated prior file\n  --model_trajs MODEL_TRAJS\n                        Comma-separated list of integers indicating which\n                        trajectories to use from the specified model. If a\n                        model is not specified, the values specified with this\n                        flag will be ignored. If a model is specified, and\n                        specific trajectories are not specified with this\n                        flag, then all trajectories will be used to inform the\n                        prior\n  --groupby &lt;string&gt;    Column name in input data file indicating those data\n                        instances that must be in the same trajectory. This is\n                        typically a subject identifier (e.g. in the case of a\n                        longitudinal data set).\n  --alpha &lt;class 'float'&gt;\n                        Dirichlet process scaling parameter. Higher values\n                        indicate belief that more trajectoreis are present.\n                        Must be a positive real value if specified.\n</code></pre> <p>This utility can be used in a number of ways. If you have no prior knowledge about how to set values for the prior, you may wish to start by running the utility with very basic information, which can later be fine-tuned. (Note that by default, the generated prior assumes no random effects).</p> <p>Let's run this utility with some basic information: the target variable we wish to analyze (y1), the predictors we wish to use (intercert and age), and our data file.</p> <pre><code>&gt; generate_prior --num_trajs 5 --preds intercept,age --targets y1 --in_data bayes_traj_tutorial_std-0.05_visits-3.csv  --out_file bayes_traj_tutorial_std-0.05_visits-3_prior_v1.p --groupby id\n</code></pre> <pre><code>Reading data...\n---------- Prior Info ----------\nalpha: 5.92e-01\n\ny1 residual (precision mean, precision variance):             (3.75e+00, 8.35e-02)\ny1 intercept (mean, std): (5.10e+00, 5.03e-01)\ny1 age (mean, std): (-2.74e-02, 7.46e-03)\n</code></pre> <p>By default, this utility makes a crude estimate of prior parameters.</p> <p>The print-out provides information about how the prior has been set. The first value, 'alpha', captures our prior belief about how many trajectories are likely to exist in our data set. This value is determined by the number specified using the --num_trajs flag. Higher alpha values indicate more trajectories are likely to be in the data and vice versa (this value is always greater than zero). Note that the actual number of trajectories will be determined during the data fitting process; the specified number of trajectories only represents an expectation.</p> <p>Next is the prior for the residual precision (1/variance). Following this are priors for the trajectory predictor coefficients.</p> <p>Is this a good prior? Does it reflect our believe about trajectories that may be in our data? This can be difficult to assess with a numerical description. For a more visual assesment, we can use the 'viz_data_prior_draws' to produce random draws from this prior and overlay these with our data. Let's first look at usage by running with the -h flag:</p> <pre><code>&gt; viz_data_prior_draws -h\n</code></pre> <pre><code>usage: viz_data_prior_draws [-h] [--data_file DATA_FILE] [--prior PRIOR]\n                            [--num_draws NUM_DRAWS] [--y_axis Y_AXIS]\n                            [--y_label Y_LABEL] [--x_axis X_AXIS]\n                            [--x_label X_LABEL] [--ylim YLIM] [--hide_resid]\n                            [--fig_file FIG_FILE]\n\nProduces a scatter plot of the data contained in the input data file as well\nas plots of random draws from the prior. This is useful to inspect whether the\nprior appropriately captures prior belief.\n\noptions:\n  -h, --help            show this help message and exit\n  --data_file DATA_FILE\n                        Input data file\n  --prior PRIOR         Input prior file\n  --num_draws NUM_DRAWS\n                        Number of random draws to take from prior\n  --y_axis Y_AXIS       Name of the target variable that will be plotted on\n                        the y-axis\n  --y_label Y_LABEL     Label to display on y-axis. If none given, the\n                        variable name specified with the y_axis flag will be\n                        used.\n  --x_axis X_AXIS       Name of the predictor variable that will be plotted on\n                        the x-axis\n  --x_label X_LABEL     Label to display on x-axis. If none given, the\n                        variable name specified with the x_axis flag will be\n                        used.\n  --ylim YLIM           Comma-separated tuple to set the limits of display for\n                        the y-axis\n  --hide_resid          If set, shaded regions corresponding to residual\n                        spread will not be displayed. This can be useful to\n                        reduce visual clutter. Only relevant for continuous\n                        target variables.\n  --fig_file FIG_FILE   File name where figure will be saved\n</code></pre> <pre><code>&gt; viz_data_prior_draws --data_file bayes_traj_tutorial_std-0.05_visits-3.csv --prior bayes_traj_tutorial_std-0.05_visits-3_prior_v1.p --num_draws 2 --x_axis age  --y_axis y1 --fig_file bayes_traj_tutorial_std-0.05_visits-3_prior_v1_draws.jpg\n</code></pre> <p></p> <p>Shown are two random draw from our prior over trajectories. The solid, colored lines represent the mean trend of randomly selected trajectories, and the shaded regions reflect the prior belief about the residual spread around each trajectory.</p> <p>There are some issues with this first-pass prior: the prior for residual variances (shaded regions), is much too large. Also, the variability in intercepts does not appear to be high enough to adequately represent our data.</p> <p>Let's rerun generate_prior, but this time will specify a higher residual precision (i.e., lower residual standard deviation value). Because we are dealing with a synthetically generated data set, we know a priori what the residual standard deviation is for each trajectory (we set it when we created the data!). In practice, you will need to use trial-and-error to select a value that best captures your belief.</p> <p>We will use the --tar_resid flag to over-ride the default residual prior settings:</p> <pre><code>&gt; generate_prior --num_trajs 5 --preds intercept,age --targets y1 --in_data bayes_traj_tutorial_std-0.05_visits-3.csv --out_file bayes_traj_tutorial_std-0.05_visits-3_prior_v2.p --groupby id --tar_resid y1,400,1e-5\n</code></pre> <pre><code>Reading data...\n---------- Prior Info ----------\nalpha: 5.92e-01\n\ny1 residual (precision mean, precision variance):             (4.00e+02, 1.00e-05)\ny1 intercept (mean, std): (5.10e+00, 5.03e-01)\ny1 age (mean, std): (-2.74e-02, 7.46e-03)\n</code></pre> <p>As before, let's visualize some random draws from this prior to see how things look:</p> <pre><code>&gt; viz_data_prior_draws --data_file bayes_traj_tutorial_std-0.05_visits-3.csv --prior bayes_traj_tutorial_std-0.05_visits-3_prior_v2.p --num_draws 20 --x_axis age --y_axis y1 --fig_file bayes_traj_tutorial_std-0.05_visits-3_prior_v2_draws.jpg\n</code></pre> <p></p> <p>Now our prior over the residual variance seems reasonable. We can further improve the prior by overriding the settings for the intercept using the --coef_std flag. Let's see how this works:</p> <pre><code>&gt; generate_prior --num_trajs 5 --preds intercept,age --targets y1 --in_data bayes_traj_tutorial_std-0.05_visits-3.csv --out_file bayes_traj_tutorial_std-0.05_visits-3_prior_v3.p --groupby id --tar_resid y1,400,1e-5 --coef_std y1,intercept,1 \n</code></pre> <pre><code>Reading data...\n---------- Prior Info ----------\nalpha: 5.92e-01\n\ny1 residual (precision mean, precision variance):             (4.00e+02, 1.00e-05)\ny1 intercept (mean, std): (5.10e+00, 1.00e+00)\ny1 age (mean, std): (-2.74e-02, 7.46e-03)\n</code></pre> <p>Now our prior over the intercept has been adjusted from a Gaussian distribution with a mean of 5.1 and a standard deviation of 0.074 to a mean of 5.1 and standard deviation of 1. Let's again look at some draws from the prior:</p> <pre><code>&gt; viz_data_prior_draws --data_file bayes_traj_tutorial_std-0.05_visits-3.csv --prior bayes_traj_tutorial_std-0.05_visits-3_prior_v3.p --num_draws 100 --hide_resid --x_axis age --y_axis y1 --fig_file bayes_traj_tutorial_std-0.05_visits-3_prior_v3_draws.jpg\n</code></pre> <p></p> <p>The increased variance for the intercept coefficient now better captures what we observe in our data. We could further tweak the prior by adjusting the coefficient for 'age' (i.e. the slope), but the trajectory subgroups are so well delineated in this idealized data set, the fitting routine should have no problem with this prior.</p> <p>Now that we have a reasonable prior, we can proceed with the actual Bayesian trajectory fitting and analysis.</p>"},{"location":"bayes_traj_tutorial/#bayesian-trajectory-analysis-continuous-target-variables","title":"Bayesian Trajectory Analysis: Continuous Target Variables","text":"<p>Now that we have generated a prior for our data set, we are ready to perform Bayesian trajectory fitting. First we demonstrate trajectory analysis in the case of continuous target variables. In this case, the algorithm assumes that the residuals around each trajectory are normally distributed.</p>"},{"location":"bayes_traj_tutorial/#bayes_traj_main","title":"bayes_traj_main","text":"<p>The fitting routine is invoked with the bayes_traj_main utility. Let's run it with the -h flag to see what inputs are required:</p> <pre><code>&gt; bayes_traj_main -h\n</code></pre> <pre><code>usage: bayes_traj_main [-h] --in_csv &lt;string&gt; --targets &lt;string&gt;\n                       [--groupby &lt;string&gt;] [--out_csv &lt;string&gt;] --prior\n                       &lt;string&gt; [--prec_prior_weight &lt;float&gt;]\n                       [--alpha &lt;class 'float'&gt;] [--out_model &lt;string&gt;]\n                       [--iters &lt;int&gt;] [--repeats &lt;int&gt;] [-k &lt;int&gt;]\n                       [--prob_thresh &lt;float&gt;] [--num_init_trajs &lt;int&gt;]\n                       [--verbose] [--probs_weight &lt;float&gt;] [--weights_only]\n\nRuns Bayesian trajectory analysis on the specified data file with the\nspecified predictors and target variables\n\noptions:\n  -h, --help            show this help message and exit\n  --in_csv &lt;string&gt;     Input csv file containing data on which to run\n                        Bayesian trajectory analysis\n  --targets &lt;string&gt;    Comma-separated list of target names. Must appear as\n                        column names of the input data file.\n\n                        instances that must be in the same trajectory. This is\n                        typically a subject identifier (e.g. in the case of a\n                        longitudinal data set).\n  --out_csv &lt;string&gt;    If specified, an output csv file will be generated\n                        that contains the contents of the input csv file, but\n                        with additional columns indicating trajectory\n                        assignment information for each data instance. There\n                        will be a column called traj with an integer value\n                        indicating the most probable trajectory assignment.\n                        There will also be columns prefixed with traj_ and\n                        then a trajectory-identifying integer. The values of\n                        these columns indicate the probability that the data\n                        instance belongs to each of the corresponding\n                        trajectories.\n  --prior &lt;string&gt;      Input pickle file containing prior settings\n  --prec_prior_weight &lt;float&gt;\n                        Positive, floating point value indicating how much\n                        weight to put on the prior over the residual\n                        precisions. Values greater than 1 give more weight to\n                        the prior. Values less than one give less weight to\n                        the prior.\n  --alpha &lt;class 'float'&gt;\n                        If specified, over-rides the value in the prior file\n  --out_model &lt;string&gt;  Pickle file name. If specified, the model object will\n                        be written to this file.\n  --iters &lt;int&gt;         Number of inference iterations\n  --repeats &lt;int&gt;       Number of repeats to attempt. If a value greater than\n                        1 is specified, the WAIC2 fit criterion will be\n                        computed at the end of each repeat. If, for a given\n                        repeat, the WAIC2 score is lower than the lowest score\n                        seen at that point, the model will be saved to file.\n  -k &lt;int&gt;              Number of columns in the truncated assignment matrix\n  --prob_thresh &lt;float&gt;\n                        If during data fitting the probability of a data\n                        instance belonging to a given trajectory drops below\n                        this threshold, then the probabality of that data\n                        instance belonging to the trajectory will be set to 0\n  --num_init_trajs &lt;int&gt;\n                        If specified, the initialization procedure will\n                        attempt to ensure that the number of initial\n                        trajectories in the fitting routine equals the\n                        specified number.\n  --verbose             Display per-trajectory counts during optimization\n  --probs_weight &lt;float&gt;\n                        Value between 0 and 1 that controls how much weight to\n\n                        observing each trajectory. This value is only\n                        meaningful if traj_probs has been set in the input\n                        prior file. Otherwise, it has no effect. Higher values\n                        place more weight on the model-derived probabilities\n                        and reflect a stronger belief in those assignment\n                        probabilities.\n  --weights_only        Setting this flag will force the fitting routine to\n                        only optimize the trajectory weights. The assumption\n                        is that the specified prior file contains previously\n                        modeled trajectory information, and that those\n                        trajectories should be used for the current fit. This\n                        option can be useful if a model learned from one\n                        cohort is applied to another cohort, where it is\n                        possible that the relative proportions of different\n                        trajectory subgroups differs. By using this flag, the\n                        proportions of previously determined trajectory\n                        subgroups will be determined for the current data set.\n</code></pre> <p>We will run the fitting routine by specifying our data set, the prior we generated, and the targets and predictors we are interested in analyzing. Also, it is important to use the groupby flag to indicate the column name in your data set that contains the subject identifier information.</p> <pre><code>&gt; bayes_traj_main --in_csv bayes_traj_tutorial_std-0.05_visits-3.csv --prior bayes_traj_tutorial_std-0.05_visits-3_prior_v3.p  --targets y1 --groupby id --iters 150 --verbose --out_model bayes_traj_tutorial_std-0.05_visits-3_model_v1.p  --num_init_trajs 5\n</code></pre> <pre><code>Reading prior...\nReading data...\nFitting...\nInitializing parameters...\niter 1, [4291.2  118.4  194.1    0.    70.  4326.2    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 2, [2354.6  303.2  122.7    0.  4046.7 2172.7    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 3, [ 580.  1873.5   45.1    0.  3468.5 3032.8    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 4, [1265.5 2778.6  318.6    0.  2302.8 2334.5    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 5, [1794.1 2847.   758.9    0.  1800.  1800.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 6, [1759.  2509.2 1131.8    0.  1800.  1800.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 7, [1794.  2370.4 1235.6    0.  1800.  1800.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 8, [1796.9 2286.3 1316.8    0.  1800.  1800.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 9, [1800.  2253.9 1346.1    0.  1800.  1800.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 10, [1800.  2233.6 1366.4    0.  1800.  1800.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\n...\niter 145, [1800.  1799.3 1800.7    0.  1800.  1800.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 146, [1800. 1799. 1801.    0. 1800. 1800.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]\niter 147, [1800.  1798.7 1801.3    0.  1800.  1800.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 148, [1800.  1798.5 1801.5    0.  1800.  1800.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 149, [1800.  1798.2 1801.8    0.  1800.  1800.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 150, [1800. 1798. 1802.    0. 1800. 1800.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]\nSaving model...\nSaving model provenance info...\n</code></pre> <p>Note that since we ran with the --verbose flag, the fitting routine prints a count corresponding to how many data instances are assigned to each trajectory, where each column corresponds to a trajectory. This is handy as it allows us to evaluate convergence of the algorithm: we should begin to see little or no change with respect to how many data instances are assigned to each trajectory, as we do here over the last few iterations. Note also that many columns are 0 or may reduce to 0. This is because the Bayesian non-parametric mechanism permits an infinite number of possible trajectories (approximated by a large, finite number in practice) and then adjusts the number of trajectories to explain the data.</p>"},{"location":"bayes_traj_tutorial/#viz_model_trajs","title":"viz_model_trajs","text":"<p>Now that we have saved the model to file, we can use another utility, viz_model_trajs, to visually inspect the results. First run with the -h flag to see the inputs:</p> <pre><code>&gt; viz_model_trajs -h\n</code></pre> <pre><code>usage: viz_model_trajs [-h] --model MODEL --y_axis Y_AXIS [--y_label Y_LABEL]\n                       --x_axis X_AXIS [--x_label X_LABEL] [--trajs TRAJS]\n                       [--min_traj_prob MIN_TRAJ_PROB]\n                       [--max_traj_prob MAX_TRAJ_PROB] [--fig_file FIG_FILE]\n                       [--traj_map TRAJ_MAP] [--xlim XLIM] [--ylim YLIM]\n                       [--hs] [--htd] [--traj_markers TRAJ_MARKERS]\n                       [--traj_colors TRAJ_COLORS] [--fill_alpha FILL_ALPHA]\n\noptions:\n  -h, --help            show this help message and exit\n  --model MODEL         Model containing trajectories to visualize\n  --y_axis Y_AXIS       Name of the target variable that will be plotted on\n                        the y-axis\n  --y_label Y_LABEL     Label to display on y-axis. If none given, the\n                        variable name specified with the y_axis flag will be\n                        used.\n  --x_axis X_AXIS       Name of the predictor variable that will be plotted on\n                        the x-axis\n  --x_label X_LABEL     Label to display on x-axis. If none given, the\n                        variable name specified with the x_axis flag will be\n                        used.\n  --trajs TRAJS         Comma-separated list of trajectories to plot. If none\n                        specified, all trajectories will be plotted.\n  --min_traj_prob MIN_TRAJ_PROB\n                        The probability of a given trajectory must be at least\n                        this value in order to be rendered. Value should be\n                        between 0 and 1 inclusive.\n  --max_traj_prob MAX_TRAJ_PROB\n                        The probability of a given trajectory can not be\n                        larger than this value in order to be rendered. Value\n                        should be between 0 and 1 inclusive.\n  --fig_file FIG_FILE   If specified, will save the figure to file.\n  --traj_map TRAJ_MAP   The default trajectory numbering scheme is somewhat\n                        arbitrary. Use this flag to provide a mapping between\n                        the defualt trajectory numbers and a desired numbering\n                        scheme. Provide as a comma-separated list of\n                        hyphenated mappings. E.g.: 3-1,18-2,7-3 would indicate\n                        a mapping from 3 to 1, from 18 to 2, and from 7 to 3.\n                        Only the default trajectories in the mapping will be\n                        plotted. If this flag is specified, it will override\n                        --trajs\n  --xlim XLIM           Comma-separated tuple to set the limits of display for\n                        the x-axis\n  --ylim YLIM           Comma-separated tuple to set the limits of display for\n                        the y-axis\n  --hs                  This flag will hide the data scatter plot\n  --htd                 This flag will hide trajectory legend details (can\n                        reduce clutter)\n  --traj_markers TRAJ_MARKERS\n                        Comma-separated list of markers to use for each\n                        trajectory. The number of markers should match the\n                        number of trajectories to renders. See matplotlib\n                        documentation for marker options\n  --traj_colors TRAJ_COLORS\n                        Comma-separated list of colors to use for each\n                        trajectory. The number of colors should match the\n                        number of trajectories to renders. See matplotlib\n                        documentation for color options\n  --fill_alpha FILL_ALPHA\n                        Value between 0 and 1 that controls opacity of each\n                        trajectorys fill region (which indicates +\\- 2\n                        residual standard deviations about the mean)\n</code></pre> <pre><code>&gt; viz_model_trajs --model bayes_traj_tutorial_std-0.05_visits-3_model_v1.p --x_axis age --y_axis y1 --fig_file bayes_traj_tutorial_std-0.05_visits-3_model_v1_fig.jpg\n</code></pre> <p></p> <p>Shown are the average trends (solid lines) identified by the algorithm together with shaded regions indicating the estimated residual precision values. Data points are color-coded based on which trajectory groub they most probably belong to.</p>"},{"location":"bayes_traj_tutorial/#trajectory-modeling-with-noisy-continuous-data","title":"Trajectory Modeling with Noisy, Continuous Data","text":"<p>Now that we have generated a trajectory model that we are happy with, we may wish to use this model to inform trajectory analysis in another data set. In this section we will analyze a much noisier data set than above. We will begin by plotting this data set.</p> <p></p> <p>This is also a synthetically generated data set, and -- except for the noise level -- has the same characteristics as the data set we worked with above (e.g. five trajectories, three \"visits\" per \"individual\" and so on). As before we start with a prior.</p>"},{"location":"bayes_traj_tutorial/#generate_prior-new-data","title":"generate_prior: new data","text":"<p>There is no reason why we couldn't use our previously generated prior straight-away to perform trajectory analysis in this data set. However, the data fitting we performed above has presumably refined our knowledge about the trajectories we are likely to encounter in this new data set. As such, we can provide the previously fit model as an input to generate_prior so that it can inform the generation of a new prior for our current data set (effectively, we are using the posterior of our previously fit model to inform the prior for our current data).</p> <p>Let's see how this works:</p> <pre><code>&gt; generate_prior --preds intercept,age --targets y1 --out_file bayes_traj_tutorial_std-0.5_visits-3_prior_v1.p --model bayes_traj_tutorial_std-0.05_visits-3_model_v1.p \n</code></pre> <pre><code>Reading model...\n---------- Prior Info ----------\nalpha: 5.92e-01\n\ny1 residual (precision mean, precision variance):             (4.00e+02, 5.95e-13)\ny1 intercept (mean, std): (4.81e+00, 4.73e-01)\ny1 age (mean, std): (-2.31e-02, 4.65e-03)\n</code></pre> <p>As before, let's visualize some random draws from this prior to see how the look with respect to our new data:</p> <pre><code>&gt; viz_data_prior_draws --data_file bayes_traj_tutorial_std-0.5_visits-3.csv --prior bayes_traj_tutorial_std-0.5_visits-3_prior_v1.p --num_draws 20 --x_axis age --y_axis y1 --fig_file bayes_traj_tutorial_std-0.5_visits-3_prior_v1_draws.jpg\n</code></pre> <p> </p> <p>There are a few points to note: * We did not explicitly define settings for the intercept or residual priors. These were gleamed from the input model. * The spread in intercept and slope appear reasonable. * The residual precision (1/variance) is apparently too high for this data.</p> <p>Given that the characteristics of our new data may be significantly different than the characteristics of the data on which the previous model was fit, we always have the option to specifically indicate what the varios prior settings should be. For example, here we have good reason to believe that the residual precision should be lower. As such, we can re-generate the prior and specifically set the residual precsion:</p> <pre><code>&gt; generate_prior --preds intercept,age --targets y1 --out_file bayes_traj_tutorial_std-0.5_visits-3_prior_v2.p --model bayes_traj_tutorial_std-0.05_visits-3_model_v1.p --groupby id --tar_resid y1,4,0.01 \n</code></pre> <pre><code>Reading model...\n---------- Prior Info ----------\nalpha: 5.92e-01\n\ny1 residual (precision mean, precision variance):             (4.00e+00, 1.00e-02)\ny1 intercept (mean, std): (4.81e+00, 4.73e-01)\ny1 age (mean, std): (-2.31e-02, 4.65e-03)\n</code></pre> <p>Again, visualize random draws from this prior to evaluate these settings:</p> <pre><code>&gt; viz_data_prior_draws --data_file bayes_traj_tutorial_std-0.5_visits-3.csv --prior bayes_traj_tutorial_std-0.5_visits-3_prior_v2.p --num_draws 2 --x_axis age --y_axis y1 --fig_file bayes_traj_tutorial_std-0.5_visits-3_prior_v2_draws.jpg\n</code></pre> <p></p> <p>Looks reasonable. Now let's see how well the trajectory fitting routine works on this data set with this prior...</p>"},{"location":"bayes_traj_tutorial/#bayes_traj_main-new-data","title":"bayes_traj_main: new data","text":"<p>Bayesian trajectory fitting proceeds exactly as before with the exception that we now set the --probs_weight flag. Since the prior was generated from a previously fit model, the prior file contains the trajectory shapes as well as the trajectory proportions from that data fit. When we use this prior on a new data set, we can tell bayes_traj_main how much weight to give to those previously determined trajectories on initialization; the fitting routine will then refine trajectory shapes and proportions using the data provided.</p> <pre><code>&gt; bayes_traj_main --in_csv bayes_traj_tutorial_std-0.5_visits-3.csv --prior bayes_traj_tutorial_std-0.5_visits-3_prior_v2.p  --targets y1 --groupby id --iters 150 --verbose --out_model bayes_traj_tutorial_std-0.5_visits-3_model_v1.p --probs_weight 1\n</code></pre> <pre><code>Reading prior...\nUsing K=30 (from prior)\nReading data...\nFitting...\nInitializing parameters...\niter 1, [1761.6 1860.4 1752.9    0.  1858.5 1766.6    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 2, [1675.2 1993.6 1643.1    0.  1986.7 1701.4    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 3, [1525.8 2189.7 1388.     0.  2172.  1724.6    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 4, [1197.  2417.7  955.1    0.  2387.5 2042.7    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 5, [ 898.2 2469.6 1148.     0.  2425.  2059.3    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 6, [ 945.5 2378.8 1416.1    0.  2296.4 1963.3    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 7, [1089.4 2287.3 1620.5    0.  2109.  1893.8    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 8, [1251.4 2216.8 1802.     0.  1882.  1847.8    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 9, [1409.  2087.4 1884.4    0.  1802.6 1816.5    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 10, [1535.7 1948.5 1797.9    0.  1918.3 1799.5    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\n...\niter 141, [1815.8 1791.1 1857.5    0.  1707.2 1828.4    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 142, [1815.7 1791.  1857.7    0.  1707.2 1828.4    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 143, [1815.6 1791.  1857.8    0.  1707.3 1828.3    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 144, [1815.6 1790.9 1857.9    0.  1707.3 1828.3    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 145, [1815.5 1790.8 1858.1    0.  1707.3 1828.3    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 146, [1815.4 1790.8 1858.2    0.  1707.3 1828.3    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 147, [1815.4 1790.7 1858.3    0.  1707.3 1828.3    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 148, [1815.3 1790.6 1858.4    0.  1707.4 1828.3    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 149, [1815.3 1790.6 1858.5    0.  1707.4 1828.2    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 150, [1815.2 1790.5 1858.7    0.  1707.4 1828.2    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\nSaving model...\nSaving model provenance info...\nDONE.\n</code></pre> <p>As before, let's visualize the trajectories:</p> <pre><code>&gt; viz_model_trajs --model bayes_traj_tutorial_std-0.5_visits-3_model_v1.p --x_axis age --y_axis y1 --fig_file bayes_traj_tutorial_std-0.5_visits-3_model_v1_fig.jpg\n</code></pre> <p></p> <p>This is a challenging data set. Although this data set was created with 5 different trajectories, the noise level is high, making it challenging to identify them. Here we benefited from a prior informed by a previously fit model. </p>"},{"location":"bayes_traj_tutorial/#new-data-multiple-dimensions","title":"New data, multiple dimensions","text":"<p>Instead of performing trajectory analysis on a single target variable (e.g. y1), we can identify trajectory subgroups by considering progression patterns in multiple dimensions, here y1 and y2. </p> <p>Let's begin by plotting both y1 and y2 vs age for our new data set.</p> <p></p> <p>As before, we begin by generating a prior.</p>"},{"location":"bayes_traj_tutorial/#generate_prior-new-data-multiple-dimensions","title":"generate_prior: new data, multiple dimensions","text":"<p>We will use our previously generated model as before. Recall that we specifically set the prior over the y1 residual precision. Given that y2 visually appears to have similar data characteristics, a reasonable place to start is to specify the same prior for the y2 residuals.</p> <pre><code>&gt; generate_prior --in_data bayes_traj_tutorial_std-0.5_visits-3.csv --preds intercept,age --targets y1,y2 --groupby id --out_file bayes_traj_tutorial_std-0.5_visits-3_prior_v3.p --tar_resid y1,4,0.01 --tar_resid y2,4,0.01\n</code></pre> <pre><code>Reading data...\n---------- Prior Info ----------\nalpha: 3.31e-01\n\ny1 residual (precision mean, precision variance):             (4.00e+00, 1.00e-02)\ny1 intercept (mean, std): (4.92e+00, 5.49e-01)\ny1 age (mean, std): (-2.47e-02, 8.20e-03)\n\ny2 residual (precision mean, precision variance):             (4.00e+00, 1.00e-02)\ny2 intercept (mean, std): (4.84e+00, 3.93e-01)\ny2 age (mean, std): (-1.56e-02, 5.76e-03)\n</code></pre> <p>Again, visualize random draws from our prior overlayed on our data. Now that we are considering multiple output dimensions, we should look at draws for both y1 and y2:</p> <pre><code>&gt; viz_data_prior_draws --data_file bayes_traj_tutorial_std-0.5_visits-3.csv --prior bayes_traj_tutorial_std-0.5_visits-3_prior_v3.p --num_draws 100 --hide_resid --x_axis age --y_axis y1 --fig_file bayes_traj_tutorial_std-0.5_visits-3_prior_v3_y1_draws.jpg\n</code></pre> <p></p> <pre><code>&gt; viz_data_prior_draws --data_file bayes_traj_tutorial_std-0.5_visits-3.csv --prior bayes_traj_tutorial_std-0.5_visits-3_prior_v3.p --num_draws 100 --x_axis age --y_axis y2 --hide_resid --fig_file bayes_traj_tutorial_std-0.5_visits-3_prior_v3_y2_draws.jpg\n</code></pre> <p></p>"},{"location":"bayes_traj_tutorial/#bayes_traj_main-new-data-multiple-dimensions","title":"bayes_traj_main: new data, multiple dimensions","text":"<p>Trajectory fitting proceeds as before. We specify the newly generated prior and also indicate that we want trajectories defined with respect to y1 and y2. </p> <pre><code>&gt; bayes_traj_main --in_csv bayes_traj_tutorial_std-0.5_visits-3.csv --prior bayes_traj_tutorial_std-0.5_visits-3_prior_v3.p  --targets y1,y2 --groupby id --iters 150 --verbose --out_model bayes_traj_tutorial_std-0.5_visits-3_model_v2.p --num_init_trajs 5\n</code></pre> <pre><code>Reading prior...\nReading data...\nFitting...\nInitializing parameters...\niter 1, [8307.1   92.   545.1   22.3   33.5    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 2, [8202.1  109.2  639.8   19.6   29.3    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 3, [7943.6  147.8  866.7   16.3   25.6    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 4, [7450.7  226.9 1289.1   12.3   21.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 5, [6877.4  347.6 1749.     9.7   16.3    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 6, [6237.1  479.2 2254.6   10.7   18.3    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 7, [5177.4  709.6 3051.7   20.7   40.7    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 8, [4408.2 1264.5 2898.7  134.8  293.8    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 9, [3679.9 1726.8 2369.3  402.4  821.7    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 10, [3440.7 1979.7 2024.7  534.6 1020.3    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\n...\niter 141, [1779.1 1817.2 1798.4 1830.9 1774.4    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 142, [1779.2 1817.  1798.4 1831.  1774.5    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 143, [1779.2 1816.9 1798.4 1831.  1774.5    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 144, [1779.2 1816.8 1798.4 1831.  1774.6    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 145, [1779.2 1816.7 1798.4 1831.1 1774.7    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 146, [1779.2 1816.6 1798.4 1831.1 1774.7    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 147, [1779.2 1816.5 1798.4 1831.1 1774.8    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 148, [1779.2 1816.4 1798.4 1831.1 1774.9    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 149, [1779.2 1816.4 1798.4 1831.2 1774.9    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 150, [1779.2 1816.3 1798.3 1831.2 1775.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\nSaving model...\nSaving model provenance info...\n</code></pre> <p>Plot the trajectories for both y1 and y2 to assess the fit:</p> <pre><code>&gt; viz_model_trajs --model bayes_traj_tutorial_std-0.5_visits-3_model_v2.p --x_axis age --y_axis y1 --fig_file bayes_traj_tutorial_std-0.5_visits-3_model_v2_y1_fig.jpg \n</code></pre> <p></p> <pre><code>&gt; viz_model_trajs --model bayes_traj_tutorial_std-0.5_visits-3_model_v2.p --x_axis age --y_axis y2 --fig_file bayes_traj_tutorial_std-0.5_visits-3_model_v2_y2_fig.jpg \n</code></pre> <p></p> <p>The addition of y2 improves the ability of the trajectory algorithm to recover the underlying population structure. </p> <p>Note that the fitting routine begins with a random initialization. For this particular data set and prior, multiple invocations of the fitting routine may result in different trajectory fit results. In practice, it is advised to generate multiple models for a given prior. Comparing these models both qualitatively and quantitatively can help identify spurious and stable trajectory subgroups.</p>"},{"location":"bayes_traj_tutorial/#model-comparison","title":"Model Comparison","text":"<p>At this point, we may be wondering if we can produce a better fit using a different predictor set. For example, what if we include age^2 as an additional predictor?</p> <p>The basic steps should now be familiar. Start by generating a prior:</p>"},{"location":"bayes_traj_tutorial/#generate_prior-new-data-multiple-dimensions-different-predictors","title":"generate_prior: new data, multiple dimensions, different predictors","text":"<pre><code>&gt; generate_prior --in_data bayes_traj_tutorial_std-0.5_visits-3.csv --preds intercept,age,age^2 --targets y1,y2 --out_file bayes_traj_tutorial_std-0.5_visits-3_prior_v4.p  --tar_resid y1,4,0.01 --tar_resid y2,4,0.01\n</code></pre> <pre><code>Reading data...\n---------- Prior Info ----------\nalpha: 2.91e-01\n\ny1 residual (precision mean, precision variance):             (4.00e+00, 1.00e-02)\ny1 intercept (mean, std): (4.86e+00, 3.21e+00)\ny1 age (mean, std): (-2.30e-02, 9.78e-02)\ny1 age^2 (mean, std): (-1.25e-05, 7.31e-04)\n\ny2 residual (precision mean, precision variance):             (4.00e+00, 1.00e-02)\ny2 intercept (mean, std): (4.91e+00, 2.29e+00)\ny2 age (mean, std): (-1.77e-02, 6.90e-02)\ny2 age^2 (mean, std): (1.59e-05, 5.10e-04)\n</code></pre> <pre><code>&gt; viz_data_prior_draws --data_file bayes_traj_tutorial_std-0.5_visits-3.csv --prior bayes_traj_tutorial_std-0.5_visits-3_prior_v4.p --num_draws 10 --x_axis age --y_axis y1 --fig_file bayes_traj_tutorial_std-0.5_visits-3_prior_v4_y1_draws.jpg\n</code></pre> <pre><code>&gt; viz_data_prior_draws --data_file bayes_traj_tutorial_std-0.5_visits-3.csv --prior bayes_traj_tutorial_std-0.5_visits-3_prior_v4.p --num_draws 5 --x_axis age --y_axis y2 --fig_file bayes_traj_tutorial_std-0.5_visits-3_prior_v4_y2_draws.jpg\n</code></pre> <p>We could further refine it as described above, but in the interest of demonstrating model comparison, it suffices.</p>"},{"location":"bayes_traj_tutorial/#bayes_traj_main-new-data-multiple-dimensions-different-predictors","title":"bayes_traj_main: new data, multiple dimensions, different predictors","text":"<pre><code>&gt; bayes_traj_main --in_csv bayes_traj_tutorial_std-0.5_visits-3.csv --prior bayes_traj_tutorial_std-0.5_visits-3_prior_v4.p --targets y1,y2 --groupby id --iters 150 --verbose --out_model bayes_traj_tutorial_std-0.5_visits-3_model_v3.p --num_init_trajs 5\n</code></pre> <pre><code>Reading prior...\nReading data...\nFitting...\nInitializing parameters...\niter 1, [4559.5 1885.   946.  1141.7  467.8    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 2, [2869.5 2590.1  803.7 1884.9  851.8    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 3, [2002.1 2649.9  849.2 2219.4 1279.3    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 4, [1260.2 2670.8 1012.9 2421.1 1635.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 5, [ 720.3 2737.5 1186.3 2508.6 1847.3    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 6, [ 567.2 2847.4 1210.9 2575.5 1799.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 7, [ 484.2 2959.3 1227.1 2604.  1725.4    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 8, [ 446.8 3048.2 1248.9 2596.6 1659.5    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 9, [ 430.4 3116.5 1271.  2572.7 1609.4    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 10, [ 425.5 3166.  1287.  2536.4 1585.2    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\n...\niter 141, [   0.  3807.4 1638.8 1823.1 1730.6    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 142, [   0.  3807.5 1638.8 1823.2 1730.5    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 143, [   0.  3807.6 1638.8 1823.2 1730.4    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 144, [   0.  3807.7 1638.8 1823.2 1730.3    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 145, [   0.  3807.7 1638.7 1823.3 1730.3    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 146, [   0.  3807.8 1638.7 1823.3 1730.2    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 147, [   0.  3807.9 1638.7 1823.3 1730.1    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 148, [   0.  3808.  1638.6 1823.4 1730.1    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 149, [   0.  3808.  1638.6 1823.4 1730.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 150, [   0.  3808.1 1638.6 1823.4 1729.9    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\nSaving model...\nSaving model provenance info...\nDONE.\n</code></pre> <pre><code>&gt; viz_model_trajs --model bayes_traj_tutorial_std-0.5_visits-3_model_v3.p --x_axis age --y_axis y1 --fig_file bayes_traj_tutorial_std-0.5_visits-3_model_v3_y1_fig.jpg \n</code></pre> <pre><code>&gt; viz_model_trajs --model bayes_traj_tutorial_std-0.5_visits-3_model_v3.p --x_axis age --y_axis y2 --fig_file bayes_traj_tutorial_std-0.5_visits-3_model_v3_y2_fig.jpg \n</code></pre> <p>The addition of the age^2 term enables representation of nonlinearity, which can be seen in these results; in this case, the nonlinearity is an overfit to the date (as we know the underlying trends in this simulated data set are linear).</p>"},{"location":"bayes_traj_tutorial/#summarize_traj_model","title":"summarize_traj_model","text":"<p>The summarize_traj_model utility allows us to inspect trajectory models quantitatively. Run with the -h flag to see usage information:</p> <pre><code>&gt; summarize_traj_model -h\n</code></pre> <pre><code>usage: summarize_traj_model [-h] --model MODEL [--trajs TRAJS]\n                            [--min_traj_prob MIN_TRAJ_PROB] [--hide_ic]\n\noptions:\n  -h, --help            show this help message and exit\n  --model MODEL         Bayesian trajectory model to summarize\n  --trajs TRAJS         Comma-separated list of integers indicating\n                        trajectories for which to print results. If none\n                        specified, results for all trajectories will be\n                        printed\n  --min_traj_prob MIN_TRAJ_PROB\n                        The probability of a given trajectory must be at least\n                        this value in order for results to be printed for that\n                        trajectory. Value should be between 0 and 1 inclusive.\n  --hide_ic             Use this flag to hide compuation and display of\n                        information criterai (BIC and WAIC2), which can take\n                        several moments to compute.\n</code></pre> <p>Lets use this utility to inspect the model that used intercept, age, and  age^2 as predictors:</p> <pre><code>&gt; summarize_traj_model --model bayes_traj_tutorial_std-0.5_visits-3_model_v3.p\n</code></pre> <pre><code>                                 Summary                                  \n==========================================================================\nNum. Trajs:         4\nTrajectories:       1,2,3,4                                 \nNo. Observations:   9000           \nNo. Groups:         3000           \nWAIC2:              32838     \nBIC1:               -16037    \nBIC2:               -15977\n\n                         Summary for Trajectory 1                         \n==========================================================================\nNo. Observations:                             3810\nNo. Groups:                                   1270\n% of Sample:                                  42.3\nOdds Correct Classification:                  49.8\nAve. Post. Prob. of Assignment:               0.97\n\n                  Residual STD       Precision Mean      Precision Var    \n--------------------------------------------------------------------------\ny1                    0.54                3.47               0.0034       \ny2                    0.61                2.69               0.0021\n\n                      coef                STD           [95% Cred. Int.]  \n--------------------------------------------------------------------------\nintercept (y1)       5.443               0.010           5.423    5.463   \nage (y1)             -0.050              0.000           -0.050  -0.050   \nage^2 (y1)           0.000               0.000           0.000    0.000   \nintercept (y2)       5.108               0.010           5.088    5.127   \nage (y2)             -0.032              0.000           -0.033  -0.032   \nage^2 (y2)           0.000               0.000           0.000    0.000\n\n\n                         Summary for Trajectory 2                         \n==========================================================================\nNo. Observations:                             1662\nNo. Groups:                                    554\n% of Sample:                                  18.5\nOdds Correct Classification:                  48.4\nAve. Post. Prob. of Assignment:               0.92\n\n                  Residual STD       Precision Mean      Precision Var    \n--------------------------------------------------------------------------\ny1                    0.52                3.65               0.0055       \ny2                    0.52                3.72               0.0057\n\n                      coef                STD           [95% Cred. Int.]  \n--------------------------------------------------------------------------\nintercept (y1)       4.277               0.013           4.252    4.303   \nage (y1)             -0.011              0.000           -0.011  -0.010   \nage^2 (y1)           0.000               0.000           0.000    0.000   \nintercept (y2)       0.014               0.013           -0.011   0.040   \nage (y2)             0.117               0.000           0.117    0.118   \nage^2 (y2)           -0.001              0.000           -0.001  -0.001\n\n\n                         Summary for Trajectory 3                         \n==========================================================================\nNo. Observations:                             1806\nNo. Groups:                                    602\n% of Sample:                                  20.1\nOdds Correct Classification:                 230.2\nAve. Post. Prob. of Assignment:               0.98\n\n                  Residual STD       Precision Mean      Precision Var    \n--------------------------------------------------------------------------\ny1                    0.50                3.95               0.0062       \ny2                    0.50                4.00               0.0064\n\n                      coef                STD           [95% Cred. Int.]  \n--------------------------------------------------------------------------\nintercept (y1)       1.234               0.012           1.210    1.257   \nage (y1)             0.048               0.000           0.048    0.049   \nage^2 (y1)           -0.001              0.000           -0.001  -0.001   \nintercept (y2)       4.727               0.012           4.704    4.751   \nage (y2)             -0.037              0.000           -0.038  -0.037   \nage^2 (y2)           0.000               0.000           0.000    0.000\n\n\n                         Summary for Trajectory 4                         \n==========================================================================\nNo. Observations:                             1722\nNo. Groups:                                    574\n% of Sample:                                  19.1\nOdds Correct Classification:                  60.9\nAve. Post. Prob. of Assignment:               0.94\n\n                  Residual STD       Precision Mean      Precision Var    \n--------------------------------------------------------------------------\ny1                    0.52                3.66               0.0054       \ny2                    0.53                3.58               0.0052\n\n                      coef                STD           [95% Cred. Int.]  \n--------------------------------------------------------------------------\nintercept (y1)       5.855               0.013           5.830    5.880   \nage (y1)             0.013               0.000           0.013    0.013   \nage^2 (y1)           -0.000              0.000           -0.000  -0.000   \nintercept (y2)       8.711               0.013           8.686    8.737   \nage (y2)             -0.075              0.000           -0.075  -0.075   \nage^2 (y2)           0.000               0.000           0.000    0.000\n</code></pre> <p>The print-out shows information for the model as a whole in the 'Summary' section at the top. Included here are three information criterion measures: BIC1, BIC2, and WAIC2. These measures reward goodness of fit while penalizing model complexity. The penalty terms is a function of N; BIC1 takes N to be the number of observations, while BIC2 takes N to be the number of groups. Generally, a higher BIC value is preferred. WAIC2 is an alternative information criterion measure which has been recommended in the Bayesian context; lower WAIC2 scores are preferred.</p> <p>Below the overall summary is per-trajectory information. This includes posterior estimates for residual precisions and predictor coefficients. Note that STD is not the same as standard error, and 95% Cred. Int. (credible interval) is not the same as a confidence interval. Rather, these are quantities that describe the Bayesian posterior distribution over these parameters. (Side note: the trajectory fitting routine uses a technique called variational inference, which is fast and scales well, but is known to underestimate posterior variances. Posterior standard deviations and credible intervals should be interpreted with this in mind). </p> <p>Also shown for each trajectory are the odds of correct classification and the average posterior probability of assignment. A rule of thumb for the odds of correct classification is that it should be greater than 5. For the average posterior probability of assignment, values of .7 or greater for each trajectory are recommended as rules of thumb.</p> <p>Now let's inspect the model generated using only intercept and age:</p> <pre><code>&gt; summarize_traj_model --model bayes_traj_tutorial_std-0.5_visits-3_model_v2.p\n</code></pre> <pre><code>                                 Summary                                  \n==========================================================================\nNum. Trajs:         5\nTrajectories:       0,1,2,3,4                               \nNo. Observations:   9000           \nNo. Groups:         3000           \nWAIC2:              27267     \nBIC1:               -13308    \nBIC2:               -13251\n\n                         Summary for Trajectory 0                         \n==========================================================================\nNo. Observations:                             1779\nNo. Groups:                                    593\n% of Sample:                                  19.8\nOdds Correct Classification:                 673.4\nAve. Post. Prob. of Assignment:               0.99\n\n                  Residual STD       Precision Mean      Precision Var    \n--------------------------------------------------------------------------\ny1                    0.50                4.03               0.0065       \ny2                    0.50                4.07               0.0066\n\n                      coef                STD           [95% Cred. Int.]  \n--------------------------------------------------------------------------\nintercept (y1)       5.975               0.012           5.951    5.998   \nage (y1)             -0.014              0.000           -0.015  -0.014   \nintercept (y2)       5.934               0.012           5.910    5.957   \nage (y2)             -0.014              0.000           -0.014  -0.014\n\n\n                         Summary for Trajectory 1                         \n==========================================================================\nNo. Observations:                             1839\nNo. Groups:                                    613\n% of Sample:                                  20.4\nOdds Correct Classification:                  62.1\nAve. Post. Prob. of Assignment:               0.94\n\n                  Residual STD       Precision Mean      Precision Var    \n--------------------------------------------------------------------------\ny1                    0.50                3.96               0.0062       \ny2                    0.50                4.02               0.0065\n\n                      coef                STD           [95% Cred. Int.]  \n--------------------------------------------------------------------------\nintercept (y1)       3.996               0.012           3.973    4.020   \nage (y1)             -0.015              0.000           -0.015  -0.015   \nintercept (y2)       3.964               0.012           3.940    3.987   \nage (y2)             -0.014              0.000           -0.014  -0.014\n\n\n                         Summary for Trajectory 2                         \n==========================================================================\nNo. Observations:                             1800\nNo. Groups:                                    600\n% of Sample:                                  20.0\nOdds Correct Classification:                 193.3\nAve. Post. Prob. of Assignment:               0.98\n\n                  Residual STD       Precision Mean      Precision Var    \n--------------------------------------------------------------------------\ny1                    0.50                4.02               0.0065       \ny2                    0.50                4.01               0.0064\n\n                      coef                STD           [95% Cred. Int.]  \n--------------------------------------------------------------------------\nintercept (y1)       3.926               0.012           3.903    3.950   \nage (y1)             -0.034              0.000           -0.034  -0.034   \nintercept (y2)       4.049               0.012           4.025    4.072   \nage (y2)             -0.016              0.000           -0.016  -0.015\n\n\n                         Summary for Trajectory 3                         \n==========================================================================\nNo. Observations:                             1827\nNo. Groups:                                    609\n% of Sample:                                  20.3\nOdds Correct Classification:                 171.0\nAve. Post. Prob. of Assignment:               0.98\n\n                  Residual STD       Precision Mean      Precision Var    \n--------------------------------------------------------------------------\ny1                    0.50                4.03               0.0065       \ny2                    0.50                3.99               0.0063\n\n                      coef                STD           [95% Cred. Int.]  \n--------------------------------------------------------------------------\nintercept (y1)       4.987               0.012           4.964    5.011   \nage (y1)             -0.015              0.000           -0.015  -0.014   \nintercept (y2)       4.960               0.012           4.937    4.984   \nage (y2)             -0.014              0.000           -0.015  -0.014\n\n\n                         Summary for Trajectory 4                         \n==========================================================================\nNo. Observations:                             1755\nNo. Groups:                                    585\n% of Sample:                                  19.5\nOdds Correct Classification:                  75.6\nAve. Post. Prob. of Assignment:               0.95\n\n                  Residual STD       Precision Mean      Precision Var    \n--------------------------------------------------------------------------\ny1                    0.50                4.03               0.0065       \ny2                    0.50                3.94               0.0062\n\n                      coef                STD           [95% Cred. Int.]  \n--------------------------------------------------------------------------\nintercept (y1)       4.992               0.012           4.968    5.016   \nage (y1)             -0.035              0.000           -0.035  -0.034   \nintercept (y2)       4.877               0.012           4.853    4.901   \nage (y2)             -0.014              0.000           -0.014  -0.013\n</code></pre> <p>As expected, the more parsimonious model (using only intercept and age as predictors) results in better information criteria scores. </p>"},{"location":"bayes_traj_tutorial/#bayesian-trajectory-analysis-binary-target-variables","title":"Bayesian Trajectory Analysis: Binary Target Variables","text":"<p>We now turn our attention to the case of binary target variables. In this case, the algorithm models the data as a mixture of logistic regressors.</p> <p>As before, we begin by printing the first few rows of a synthetically generated data set that containts two binary target variables (y1 and y2) and a time variable, x. In this data set, there are four distinct trajectories, with 200 individuals in each trajectory and 10 time points per individual. 'sid' is the subject identifier column, 'traj_gt' indicates the ground-truth trajectory assignment, and 'intercept' is a column of 1s. The four trajectories were generated to have the following probability patterns with respect to the time variable, x: * Trajectory 1 -- y1: decreasing, y2: decreasing, shifted left * Trajectory 2 -- y1: decreasing, y2: decreasing, shifted right * Trajectory 3 -- y1: increasing, y2: decreasing, shifted left * Trajectory 4 -- y1: increasing, y2: decreasing, shifted right</p> sid mu1 mu2 y1 y2 intercept x traj_gt 0 0.0 0.994590 0.999554 1 1 1.0 -5.214084 0.0 1 0.0 0.976913 0.998064 1 1 1.0 -3.745119 0.0 2 0.0 0.939964 0.994785 1 1 1.0 -2.750904 0.0 3 0.0 0.789713 0.978610 1 1 1.0 -1.323194 0.0 4 0.0 0.699091 0.965874 1 1 1.0 -0.842974 0.0 5 0.0 0.362511 0.873859 1 1 1.0 0.564483 0.0 6 0.0 0.153625 0.688593 0 0 1.0 1.706451 0.0 7 0.0 0.061672 0.444659 0 1 1.0 2.722275 0.0 8 0.0 0.021875 0.214112 0 0 1.0 3.800312 0.0 9 0.0 0.005367 0.061681 0 0 1.0 5.222111 0.0 10 1.0 0.993601 0.999472 1 1 1.0 -5.045176 0.0 11 1.0 0.978141 0.998169 1 1 1.0 -3.801039 0.0 12 1.0 0.935340 0.994357 1 1 1.0 -2.671766 0.0 13 1.0 0.834289 0.983957 1 0 1.0 -1.616334 0.0 14 1.0 0.577248 0.943293 0 1 1.0 -0.311488 0.0"},{"location":"bayes_traj_tutorial/#prior-generation","title":"Prior Generation","text":"<p>The procedure for generating priors in this case proceeds as above. Here we will specifically set the priors over the predictor coefficients to be zero-centered Gaussian distributions with unit variance:</p> <pre><code>&gt; generate_prior --num_trajs 4 --preds intercept,x --targets y1,y2 --in_data binary_data_4.csv  --coef y1,intercept,0,1 --coef y1,x,0,1 --coef y2,intercept,0,1 --coef y2,x,0,1 --out_file binary_data_4_prior.p --groupby sid\n</code></pre> <pre><code>Reading data...\nOptimization terminated successfully.\n         Current function value: 0.693140\n         Iterations 3\nOptimization terminated successfully.\n         Current function value: 0.466741\n         Iterations 6\n---------- Prior Info ----------\nalpha: 5.51e-01\n\ny1 intercept (mean, std): (0.00e+00, 1.00e+00)\ny1 x (mean, std): (0.00e+00, 1.00e+00)\n\ny2 intercept (mean, std): (0.00e+00, 1.00e+00)\ny2 x (mean, std): (0.00e+00, 1.00e+00)\n</code></pre> <p>As before, let's visualize draws from this prior:</p> <pre><code>!viz_data_prior_draws --data_file binary_data_4.csv --prior binary_data_4_prior.p --num_draws 20 --x_axis x  --y_axis y1 --fig_file binary_data_4_prior_draws.jpg\n</code></pre> <p></p>"},{"location":"bayes_traj_tutorial/#model-fitting-and-visualization","title":"Model Fitting and Visualization","text":"<p>Model fitting proceeds as in the continuous case. Here we will fit using both target variables.</p> <pre><code>!bayes_traj_main --in_csv binary_data_4.csv --prior binary_data_4_prior.p --targets y1,y2 --groupby sid --out_model binary_data_4_model.p --verbose --iters 50 --alpha .55\n</code></pre> <pre><code>Reading prior...\nReading data...\nFitting...\nInitializing parameters...\niter 1, [6560.7 1215.2  117.9  106.2    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 2, [5700.8 2099.2  122.8   77.2    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 3, [4643.9 3260.5   60.5   35.1    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 4, [4188.2 3676.3   44.6   90.9    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 5, [3480.1 3552.   216.9  751.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 6, [2133.  2335.3 1259.6 2272.1    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 7, [1866.6 1512.1 2064.7 2556.7    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 8, [1906.2 1713.3 2093.3 2287.2    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 9, [1928.2 1841.2 2071.8 2158.9    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 10, [1938.5 1903.7 2061.5 2096.3    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\n...\niter 41, [1964.6 1998.9 2035.4 2001.1    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 42, [1965.8 1996.8 2034.2 2003.2    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 43, [1961.2 1993.5 2038.8 2006.5    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 44, [1967.  1998.8 2033.  2001.2    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 45, [1961.  1998.4 2039.  2001.6    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 46, [1963.5 1999.  2036.5 2001.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 47, [1963.4 1995.4 2036.6 2004.6    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 48, [1967.7 1993.6 2032.3 2006.4    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 49, [1962.3 2000.8 2037.7 1999.2    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\niter 50, [1962.8 1992.9 2037.2 2007.1    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0. ]\nSaving model...\nSaving model provenance info...\nDONE.\n</code></pre> <p>Finally, let's visualize the results and inspect the y1 and y2 trends:</p> <pre><code>!viz_model_trajs --model binary_data_4_model.p --x_axis x --y_axis y1 --fig_file binary_data_4_model_y1_fig.jpg --traj_markers o,s,^,d\n</code></pre> <p></p> <pre><code>!viz_model_trajs --model binary_data_4_model.p --x_axis x --y_axis y2 --fig_file binary_data_4_model_y2_fig.jpg --traj_markers o,s,^,d\n</code></pre> <p></p> <p>The detected trajectories capture the underlying trends. Note that the trajectory numbers assigned by the algorithm are arbitrary.</p>"},{"location":"formulation/","title":"Formulation","text":"<p>Here we describe our Bayesian formulation, beginning with a brief review of Dirichlet process mixtures, a key element of our model.</p>"},{"location":"formulation/#dirichlet-process-mixtures","title":"Dirichlet Process Mixtures","text":"<p>Ferguson (1973) first introduced the Dirichlet process (DP) as a measure on measures. It is parameterized by a base measure, \\(\\mathit{G}_0\\), and a positive scaling parameter \\( \\alpha \\): $$   \\mathit{G} \\vert \\{ \\mathit{G}_0, \\alpha \\} \\sim \\text{DP}\\left( \\mathit{G}_0, \\alpha \\right) $$ The notion of a Dirichlet process mixture (DPM) arises if we treat the \\( k^{th} \\) draw from \\( \\mathit{G} \\) as a parameter of the distribution over some observation (Antoniak (1974)). DPMs can be interpreted as mixture models with an infinite number of mixture components.</p> <p>More recently, Blei and Jordan (2006) described a variational inference algorithm for DPMs using the stick-breaking construction introduced in Sethuraman (1991). The stick-breaking construction represents \\( G \\) as $$   \\pi_{k}( \\mathbf{v} ) = \\mathbf{v}_{k}\\prod^{k-1}_{j=1}\\left( 1-\\mathbf{v}_{j}\\right) $$</p> <p>$$ \\mathit{G} = \\sum_{i=1}^{\\infty}\\pi_{i}\\left( \\mathbf{v}\\right)\\delta_{\\eta_{i}^{\\ast}}  $$ where \\( \\delta_{\\eta_{i}^{\\ast}}   \\) is the Kronecker delta, and the \\( \\mathbf{v}_{i} \\) are distributed according to a beta distribution: \\( \\mathbf{v}_{i} \\sim \\text{Beta}\\left( 1, \\alpha \\right) \\), and  \\( \\eta_{i}^{\\ast} \\sim \\mathit{G}_0 \\). We use a DPM in our model to automatically identify the number of trajectories that best explain our data.</p>"},{"location":"formulation/#model-formulation","title":"Model Formulation","text":"<p>We consider a collection of multiple longitudinally observed target variables, which can be continuous, binary, or a combination. We let \\(y_{g,i,d}\\) represent the observation for individual \\(g\\) \\( (g=1,\\dots,G) \\) at the \\( i^{th} \\) occasion \\( (i=1,\\dots,n_g) \\) for target variable \\(d\\).  Similarly, \\(x_{g,i,m}\\) represents predictor \\(m\\) \\( (m=1,\\dots,M)\\) for individual \\(g\\) on occasion \\(i\\). Here, \\(G\\) is the total number of individuals in the data sample, \\(n_g\\) is the number of observations per individual, and \\(M\\) is the number of predictors. The likelihood in our formulation factorizes into two terms: $$ p(\\mathbf{Y}\\mid \\mathbf{Z}, \\mathbf{W}, \\boldsymbol{\\lambda}, \\mathbf{U}) = p(\\mathbf{Y}_c\\mid \\mathbf{Z}, \\mathbf{W}_c, \\boldsymbol{\\lambda}, \\mathbf{U}) p(\\mathbf{Y}_b\\mid \\mathbf{Z}, \\mathbf{W}_b) $$ where we distinguish between the collection of \\(D_c\\) continuous target variables, \\(Y_c\\), and the collection of \\(D_b\\) binary target variables, \\(Y_b\\). The likelihood factors are given by:</p> <p>$$ p(\\mathbf{Y}_c\\mid \\mathbf{Z}, \\mathbf{W}_c, \\boldsymbol{\\lambda}, \\mathbf{U})= \\prod_{k=1}^{\\infty} \\prod_{g=1}^{G}\\prod_{d_c=1}^{D_c}\\left[ \\prod_{i=1}^{n_g}\\mathcal{N}\\left(y_{g,i,d_c} \\mid (\\mathbf{w}_{\\cdot, d_c, k} + \\mathbf{u}_{g, d_c, k})^{T} \\mathbf{x}_{g,i,\\cdot}, \\lambda_{d_c,k}^{-1}\\right)\\right]^{z_{g, k}} $$ and $$ p(\\mathbf{Y}_b\\mid \\mathbf{Z}, \\mathbf{W}_b)=\\prod_{k=1}^{\\infty}\\prod_{g=1}^{G} \\prod_{d_b=1}^{D_b}  \\left[\\prod_{i=1}^{n_g} \\frac{\\exp(\\mathbf{w}_{\\cdot, d_b, k}^{T} \\mathbf{x}_{g,i,\\cdot})^{y_{g,i,d_b}}} {1+\\exp(\\mathbf{w}_{\\cdot, d_b, k}^{T} \\mathbf{x}_{g,i,\\cdot})} \\right]^{z_{g,k}}. \\label{YbLike} $$</p> <p>We formulate our model as a DPMM, which can be interpreted as a mixture model with a potentially infinite number of mixture components (Antoniak (1974)). The \\(G \\times \\infty\\) binary indicator matrix, \\(\\mathbf{Z}\\), represents the association between subjects and the potentially infinite number of latent regression functions (trajectories), and \\(k\\) represents the group membership for each individual. In the case of \\(\\mathbf{Y}_c\\) this formulation can be see as mixture of linear regressors, and in the case of \\(\\mathbf{Y}_b\\) it can be seen as an infinite mixture of logistic regressors.  </p> <p>\\( \\mathbf{W}_c \\) represents the \\(M \\times D_c \\times \\infty\\) matrix of predictor coefficients for the linear regressors, and  \\( \\mathbf{W}_b \\) represents the \\(M \\times D_b \\times \\infty\\) matrix of predictor coefficients for the logistic regressors. We put Gaussian priors over both \\( \\mathbf{W}_c\\) and \\(\\mathbf{W}_b\\), where \\( \\boldsymbol{\\mu}_0 \\) and \\( \\boldsymbol{\\lambda}_0 \\) capture practioner believe about coeffient values:</p> <p>$$ p\\left(\\mathbf{W}_c \\right)= \\prod_{m=1}^{M} \\prod_{d_c=1}^{D_c} \\prod_{k=1}^{\\infty} \\mathcal{N}\\left(w_{m, d_c, k} \\mid \\mu_{0_{c_{m, d_c}}}, \\lambda_{0_{c_{m, d_c}}}^{-1}\\right) $$</p> <p>and $$ p\\left(\\mathbf{W}_b \\right)= \\prod_{m=1}^{M} \\prod_{d_b=1}^{D_b} \\prod_{k=1}^{\\infty} \\mathcal{N}\\left(w_{m, d_b, k} \\mid \\mu_{0_{b_m, d_b}}, \\lambda_{0_{b_m, d_b}}^{-1}\\right) \\label{Wbprior} $$</p> <p>\\( \\mathbf{U} \\) represents the \\(G \\times D_c \\times \\infty\\) matrix of (optional) random effects for the continuous target variables:</p> <p>$$ p\\left(\\mathbf{U}\\right)= \\prod_{g=1}^{G} \\prod_{d=1}^{D_c} \\prod_{k=1}^{\\infty} \\mathcal{N}\\left(\\mathbf{u}_{g, d_c, k} \\mid \\mathbf{0}, \\mathbf{\\Sigma_0} \\right) $$</p> <p>(Note that the dimension length of random effect vectors is generally less than \\( \\mathbf{M} \\), the number of predictors. Elements of \\( \\mathbf{u}_{g, d_c, k} \\) corresponding to predictors with no random effects are set to 0.) Here we assume that the random effects have mean \\( \\mathbf{0} \\), and the unstructured covariance matrix \\( \\boldsymbol{\\Sigma}_0\\) captures prior believe about predictor variability within a trajectory subgroup.</p> <p>We learn the residual precisions, \\( \\boldsymbol{\\lambda} \\), for each of the \\(D_c \\times \\infty\\) linear regressors, and place gamma priors over these terms: $$ p(\\boldsymbol{\\lambda})= \\prod_{k=1}^{\\infty} \\prod_{d_c=1}^{D_c} \\mathrm{Gam}\\left(\\lambda_{d_c, k} \\mid a_{0_{d_c}}, b_{0_{d_c}}\\right). $$</p> <p>The nonparametric prior distribution over \\( \\mathbf{Z} \\) is given by: $$ p(\\mathbf{Z} \\mid \\mathbf{v})= \\prod_{g=1}^{G} \\prod_{k=1}^{\\infty}\\left(v_{k} \\prod_{j=1}^{k-1}\\left(1-v_{j}\\right)\\right)^{z_{g, k}} $$ This can be considered a \\( G\\times \\infty \\) multinomial distribution with parameters drawn for a DP using the stick-breaking construction (see Blei 2006 and Sethuraman 1994 for details), where the elements of \\(\\mathbf{v}\\) are drawn from a beta distribution with concentration parameter \\(\\alpha\\): $$ p(\\mathbf{v})= \\prod_{k=1}^{\\infty} \\mathrm{Beta}\\left(v_{k} \\mid 1, \\alpha\\right). \\label{prior_v} $$ The concentration parameter \\(\\alpha\\) captures the practitioner\u2019s prior belief about whether there are fewer groups (low scale parameter value) or more groups (larger scale parameter value). A benefit of the non-parametric framework is that the number of components that best describe the observed data is automatically determined conditioned on this value.</p> <p>With these terms defined, the joint density is given as: $$ p\\left(\\mathbf{Y}_c,  \\mathbf{Y}_b,  \\mathbf{W}_c,  \\mathbf{W}_b,  \\boldsymbol{\\lambda},  \\mathbf{Z},  \\mathbf{v}, \\mathbf{U},  \\mid  \\mathbf{X}, \\boldsymbol{\\mu}_{c},  \\boldsymbol{\\lambda}_c,  \\boldsymbol{\\mu}_b,  \\boldsymbol{\\lambda}_b,  \\mathbf{a}_0,  \\mathbf{b}_0,  \\alpha, \\mathbf{\\Sigma}_0,  \\right) = $$ $$ p(\\mathbf{Y}_c \\mid \\mathbf{Z}, \\mathbf{W}_c, \\boldsymbol{\\lambda}, \\mathbf{U}) p(\\mathbf{Y}_b \\mid \\mathbf{Z}, \\mathbf{W}_b) p\\left(\\mathbf{W}_c \\mid \\boldsymbol{\\mu}_c, \\boldsymbol{\\lambda}_c\\right) p\\left(\\mathbf{W}_b \\mid \\boldsymbol{\\mu}_b, \\boldsymbol{\\lambda}_{b}\\right) $$ $$ p(\\boldsymbol{\\lambda} \\mid \\mathbf{a}_0, \\mathbf{b}_0) p(\\mathbf{Z} \\mid \\mathbf{v})  p(\\mathbf{v} \\mid \\alpha) $$</p> <p>In the Inference section, we describe how the posterior distribution over the latent variables is estimated using coordinate ascent variational inference.</p>"},{"location":"inference/","title":"Variational Inference","text":""},{"location":"inference/#overview","title":"Overview","text":"<p>Frequently with complex models, obtaining the exact posterior distribution can be intractable. While Markov Chain Monte Carlo (MCMC)  methods offer a systematic approach to sample from the posterior distribution, they can be slow in high-dimensional parameter spaces (Hastings (1970)). An alternative is variational inference, a form of Bayesian approximate inference that tends to be fast and scales well to large data sets (Jordan (1999)). It typically makes a factorization assumption over the approximate posterior distribution of interest, and it turns an inference problem into an optimization problem by finding the approximate posterior that minimizes the Kullback-Leibler divergence to the true posterior distribution  (see Jordan (1999), Jaakkola (2001), and Blei (2017)).</p> <p>In the case of conjugate priors (a prior is conjugate when the posterior distribution belongs to the same family of probability distributions as the prior distribution given a specific likelihood function), there is a straightforward procedure for deriving variational update eqautions. In the case of non-conjugate priors, alternative approaches are needed.</p> <p>For our model, the posterior distribution we wish to estimate is given by: $$ p\\left(\\mathbf{W}_c, \\mathbf{W}_b, \\boldsymbol{\\lambda}, \\mathbf{Z}, \\mathbf{v}, \\mathbf{U} \\mid \\mathbf{Y}_c, \\mathbf{Y}_b, \\mathbf{X}, \\boldsymbol{\\mu}_{0_c}, \\boldsymbol{\\lambda}_{0_c}, \\boldsymbol{\\mu}_{0_b}, \\boldsymbol{\\lambda}_{0_b}, \\mathbf{a}_0, \\mathbf{b}_0, \\alpha, \\boldsymbol{\\Sigma}_0 \\right) $$ This posterior probability is approximated using variational inference. The standard mean field variational inference approach is to assume a factorized approximation of this distribution, in our case: $$ p^*(\\mathbf{W}_{c}) p^*(\\mathbf{W}_{b}) p^*(\\boldsymbol{\\lambda}) p^*(\\mathbf{Z}) p^*(\\mathbf{v}) p^*(\\mathbf{U}) $$ In order to derive the expression for each of these factors, the expectation with respect to the other factors is considered. Derivation of the variational distributions begins with the following expressions: $$ \\text{ln}p^*(\\mathbf{W}_c) = \\mathbb{E}_{\\mathbf{W}_b, \\mathbf{\\lambda}, \\mathbf{Z}, \\mathbf{v}, \\mathbf{U} }\\{\\text{ln}p(\\mathbf{Y}_c, \\mathbf{Y}_b, \\mathbf{W}_c, \\mathbf{W}_b, \\boldsymbol{\\lambda}, \\mathbf{Z}, \\mathbf{v}, \\mathbf{U} ) \\} + \\text{const} $$</p> <p>$$ \\text{ln}p^*(\\mathbf{U}) = \\mathbb{E}_{ \\mathbf{W}_c, \\mathbf{W}_b, \\mathbf{\\lambda}, \\mathbf{Z}, \\mathbf{v} }\\{\\text{ln}p(\\mathbf{Y}_c, \\mathbf{Y}_b, \\mathbf{W}_c, \\mathbf{W}_b, \\boldsymbol{\\lambda}, \\mathbf{Z}, \\mathbf{v}, \\mathbf{U} ) \\} + \\text{const} $$</p> <p>$$ \\text{ln}p^*(\\boldsymbol{\\lambda}) = \\mathbb{E}_{ \\mathbf{W}_c, \\mathbf{W}_b, \\mathbf{Z}, \\mathbf{v}, \\mathbf{U} }\\{ \\text{ln}p(\\mathbf{Y}_c, \\mathbf{Y}_b, \\mathbf{W}_c, \\mathbf{W}_b, \\boldsymbol{\\lambda}, \\mathbf{Z}, \\mathbf{v}, \\mathbf{U} )\\} + \\text{const} $$</p> <p>$$ \\text{ln}p^*(\\mathbf{v}) = \\mathbb{E}_{ \\mathbf{W}_c, \\mathbf{W}_b, \\boldsymbol{\\lambda}, \\mathbf{Z}, \\mathbf{U} } \\{\\text{ln}p(\\mathbf{Y}_c, \\mathbf{Y}_b, \\mathbf{W}_c, \\mathbf{W}_b, \\mathbf{\\lambda}, \\mathbf{Z}, \\mathbf{v}, \\mathbf{U} ) \\} + \\text{const} $$</p> <p>$$ \\text{ln}p^*(\\mathbf{Z}) = \\mathbb{E}_{ \\mathbf{W}_c, \\mathbf{W}_b, \\boldsymbol{\\lambda}, \\mathbf{v}, \\mathbf{U} } \\{\\text{ln}p(\\mathbf{Y}_c, \\mathbf{Y}_b, \\mathbf{W}_c, \\mathbf{W}_b, \\boldsymbol{\\lambda}, \\mathbf{Z}, \\mathbf{v}, \\mathbf{U} )\\} + \\text{const} $$</p> <p>A challenge arises, however, if priors in the model are not conditionally conjugate \u2013 i.e. if factor posteriors are not in the same family as the corresponding priors. This is the case with the Gaussian priors for the coefficients of logistic regression \\(\\mathbf{W}_b\\), meaning that the distribution \\(p^*(\\mathbf{W}_b) \\) can not be assumed Gaussian unless approximations are made to restore conjugacy. We address this challenge by integrating the coordinate ascent variational inference algorithm with the EM (Expectation-Maximization) algorithm to facilitate updates of \\( p^*(\\mathbf{W}_b) \\) (see below).</p>"},{"location":"inference/#variational-distributions","title":"Variational Distributions","text":"<p>Here we provide expressions for each of the variational distributions.</p>"},{"location":"inference/#variational-distribution-pmathbfw_c","title":"Variational distribution \\( p^*(\\mathbf{W}_c) \\)","text":"<p>The variational distribution over the coefficients \\( \\mathbf{W}_c \\) is given by a multivariate Gaussian distribution that factorizes over the predictors, targets, and subtype clusters:</p> <p>$$ p^*(\\mathbf{W}_c)= \\prod_{m=1}^{M} \\prod_{d_c=1}^{D_c} \\prod_{k=1}^{K} \\mathcal{N} \\left( w_{m, d_c, k} \\mid \\mu_{m, d_c, k}, {\\lambda^{-1}_{m, d_c, k}} \\right) $$</p> <p>$$ \\lambda_{m, d_c, k}= \\lambda_{0_{c_{m, d_c}}}+ \\frac{a_{d_c, k}}{b_{d_c, k}} \\sum_{g=1}^{G} \\mathbb{E}_{\\mathbf{z}}\\{z_{g, k}\\} \\sum_{i=1}^{n_g} x_{i,m}^2 $$</p> <p>$$ \\mu_{m, d_c, k}= {\\lambda_{m, d_c, k}}^{-1} \\bigg[ \\mu_{0_{c_{m, d_c}}} {\\lambda_{0_{c_{m, d_c}}}} - \\frac{{a_{d_c, k}}}{{b_{d_c, k}}} \\sum_{g=1}^{G} \\mathbb{E}_{\\mathbf{z}}\\{z_{g, k}\\} \\times $$ $$ \\sum_{i=1}^{n_g}x_{g,i,m} \\left(\\mathbb{E}_{\\mathbf{w}}\\{\\mathbf{w}_{-, d_c, k}\\}^{T} \\ \\mathbf{x}_{g,i,-} + \\mathbb{E}\\{ \\mathbf{u}_{g,d_c,k} \\}^T \\mathbf{x}_{g,i,.} - y_{g,i, d_c}\\right) \\bigg]  \\label{Wc_ast} $$</p> <p>The \\(-\\) in \\( \\mathbf{w}_{-,d_c,k} \\) and \\( \\mathbf{x}_{g,i,-} \\) indicates all but the \\(m^{th}\\) predictor.</p>"},{"location":"inference/#variational-distribution-pmathbfu","title":"Variational distribution \\( p^{*}(\\mathbf{U}) \\)","text":"<p>The variational distribution for \\( p^{*}(\\mathbf{U}) \\) is given by</p> <p>$$ p^{*}(\\mathbf{U}) = \\prod_{g=1}^{G} \\prod_{d_c=1}^{D_c} \\prod_{k=1}^{K} \\mathcal{N} \\left( \\mathbf{u}_{g, d_c, k} \\mid \\boldsymbol{\\mu}_{g, d_c, k}, \\boldsymbol{\\Sigma}_{g, d_c, k} \\right) $$ where $$ \\boldsymbol{\\Sigma}_{g, d_c, k} = \\left[ \\boldsymbol{\\Sigma}_0^{-1} +  \\mathbb{E}\\{ z_{g,k}\\} \\frac{a_{d_c,k}}{b_{d_c,k}} \\sum_{i=1}^{n_g} \\mathbf{x}_{g,i,d_c}\\mathbf{x}_{g,i,d_c}^T \\right]^{-1} $$ and $$ \\boldsymbol{\\mu}_{g, d_c, k} = \\left[ \\mathbb{E}\\{ z_{g,k}\\} \\frac{a_{d_c,k}}{b_{d_c,k}} \\sum_{i=1}^{n_g} \\left( \\mathbb{E}\\{  w_{., d, k}  \\}^T \\mathbf{x}_{g,i,d_c} - y_{g, i, d_c} \\right)\\mathbf{x}_{g,i,d_c}^T \\right] \\boldsymbol{\\Sigma}_{g, d_c, k} $$</p>"},{"location":"inference/#variational-distribution-pboldsymbollambda","title":"Variational distribution \\( p^{*}(\\boldsymbol{\\lambda}) \\)","text":"<p>The variational distribution over \\( \\mathbf{\\lambda} \\) is given by a gamma distribution with parameters \\( \\mathbf{a} \\) and \\( \\mathbf{b} \\): $$ p^{*}(\\boldsymbol{\\lambda})= \\prod_{d_c=1}^{D_c} \\prod_{k=1}^{K} \\operatorname{Gam} \\left(\\lambda_{d_c, k} \\mid a_{d_c, k}, b_{d_c, k} \\right) $$</p> <p>$$ a_{d_c, k}= a_{0_{d_c}}+ \\frac{1}{2} \\sum_{g=1}^{G}n_g \\mathbb{E}_{\\mathbf{z}}\\{z_{g, k}\\}  $$</p> <p>$$ b_{d_c, k}= b_{0_{d_c}}+ \\frac{1}{2} \\sum_{g=1}^{G} \\sum_{i=1}^{n_g} \\mathbb{E}\\{z_{g, k}\\} \\bigg(\\mathbb{E}\\{ \\left({\\mathbf{w}^{T}_{\\cdot, d_c, k}} \\mathbf{x}_{g,i, \\cdot} \\right)^{2}\\} -2 y_{g,i,d_c} \\mathbb{E}\\{\\mathbf{w}_{\\cdot,d_c, k}\\}^{T} \\mathbf{x}_{g,i,\\cdot} + {y^{2}_{g,i, d_c}} $$ $$ -2y_{g,i,d_c}\\mathbb{E}\\{ \\mathbf{u}_{g,d_c,k}\\}^T\\mathbf{x}_{g,i,.} + 2\\mathbb{E}\\{ \\mathbf{w}_{.,d_c,k}\\}^T\\mathbf{x}_{g,i,.} \\mathbb{E}\\{ \\mathbf{u}_{g,d_c,k}\\}^T\\mathbf{x}_{g,i,.} + \\mathbb{E}\\{ ( \\mathbf{u}_{g,d_c,k}^T\\mathbf{x}_{g,i,.} )^2 \\} \\bigg) \\label{lambda_ast} $$</p>"},{"location":"inference/#variational-distribution-pmathbfv","title":"Variational distribution \\( p^{*}(\\mathbf{v}) \\)","text":"<p>The variational distribution for \\( p^{*}(\\mathbf{v}) \\) is given by $$ p^{*}(\\mathbf{v})= \\prod_{k=1}^{K} \\text{Beta} \\left(  v_k \\mid 1 + \\sum_{g=1}^{G} \\mathbb{E}\\{\\mathbf{Z} \\}_{g, k},  \\alpha + \\sum_{j=k+1}^{K} \\sum_{g=1}^{G} \\mathbb{E}\\{\\mathbf{Z} \\}_{g, j} \\right) \\label{v_ast} $$ where \\( K \\) is an integer (e.g. 20) chosen by the user for the truncated stick-breaking process.</p>"},{"location":"inference/#variational-distribution-pmathbfz","title":"Variational distribution \\( p^{*}(\\mathbf{Z}) \\)","text":"<p>The variational distribution for \\( p^{*}(\\mathbf{Z}) \\) is given by</p> <p>$$ p^{*}(\\mathbf{Z})= \\prod_{g=1}^{G}\\prod_{k=1}^{K}r_{g,k}^{z_{g,k}} \\label{pZstar} $$</p> <p>where</p> <p>$$ r_{g,k}=\\frac{\\rho_{g,k}}{\\sum_{k=1}^{K}\\rho_{g,k}} \\label{eq:rnk} $$</p> <p>and</p> <p>$$ \\ln \\rho_{g, k} = n_g\\mathbb{E}\\{\\ln v_k\\} + n_g\\sum_{j=1}^{k-1}\\mathbb{E}\\{\\ln (1-v_j)\\} + \\ $$</p> <p>$$ \\frac{1}{2}\\sum_{d_c=1}^{D_c} \\bigg[n_g\\mathbb{E}\\{\\ln \\lambda_{d_c, k}\\}-n_g\\ln (2 \\pi) - $$ $$ \\mathbb{E}\\{\\lambda_{d_c, k}\\} \\sum_{i=1}^{n_g} \\bigg(\\mathbb{E}\\{\\left({\\mathbf{w}^{T}_{\\cdot, d_c, k}} \\mathbf{x}_{g,i,\\cdot}\\right)^2\\} - 2 y_{g,i,d_c} \\mathbb{E}\\{{\\mathbf{w}_{\\cdot, d_c, k}}\\}^{T} \\mathbf{x}_{g,i,\\cdot}+y^2_{g,i, d_c} + $$</p> <p>$$ -2y_{g,i,d_c}\\mathbb{E}\\{ \\mathbf{u}_{g,d_c,k}\\}^T\\mathbf{x}_{g,i,.} + 2\\mathbb{E}\\{ \\mathbf{w}_{.,d_c,k}\\}^T\\mathbf{x}_{g,i,.} \\mathbb{E}\\{ \\mathbf{u}_{g,d_c,k}\\}^T\\mathbf{x}_{g,i,.} + \\mathbb{E}\\{ ( \\mathbf{u}_{g,d_c,k}^T\\mathbf{x}_{g,i,.} )^2 \\} \\bigg)\\bigg] + $$ $$ \\sum_{d_b=1}^{D_b} \\sum_{i=1}^{n_g} \\left[ y_{g,i, d_b} \\mathbb{E}\\{     {\\mathbf{w}_{\\cdot, d_b, k}}\\}^{T}{\\mathbf{x}_{g,i, \\cdot}}     -\\mathbb{E}\\{\\ln\\left(1+\\exp\\left(\\mathbf{x}_{g,i, \\cdot}\\cdot{\\mathbf{w}_{\\cdot, d_b, k}}^T\\right)\\right)\\}     \\right] $$</p>"},{"location":"inference/#update-strategy-for-mathbfw_b","title":"Update Strategy for \\( \\mathbf{W}_b \\)","text":"<p>As mentioned above, the Gaussian priors over the coefficients \\( \\mathbf{W}_b \\) are not conjugate concerning the likelihood factor \\( p(\\mathbf{Y}_b\\mid \\mathbf{Z}, \\mathbf{W}_b) \\). When the prior and likelihood are not conjugate, Bayesian inference becomes more complex and computationally demanding since the posterior distribution cannot be derived analytically.  Our methodology applies a tangent quadratic lower bound to the logistic likelihoods within the framework of variational inference for conditionally conjugate exponential family models  (see Durante (2019)). This approach restores conjugacy between the approximate bounds and the Gaussian priors on \\( \\mathbf{W}_b \\).</p> <p>Jaakkola and Jordan (2000) introduced a straightforward variational approach based on a family of tangent quadratic lower bounds of logistic log-likelihoods. They derived an EM algorithm to iteratively refine the variational parameters of the lower bound and the mean and covariance of the Gaussian distribution over the predictor coefficients. However, this method was specifically designed for simple logistic regression and did not extend to mixtures of logistic regressors. To address this, we extend these concepts to Dirichlet Process mixture models in our formulation. We can augment the likelihood function \\( p(\\mathbf{Y}_b\\mid \\mathbf{Z}, \\mathbf{W}_b) \\):</p> <p>$$ p(\\mathbf{Y}_b, \\boldsymbol{\\zeta} \\mid \\mathbf{Z}, \\mathbf{W}_b)= \\prod_{k=1}^{\\infty}\\prod_{g=1}^{G} \\prod_{d_b=1}^{D_b}  \\left[\\prod_{i=1}^{n_g} p\\left( y_{g,i,d_b}\\vert \\mathbf{w}_{\\cdot,d_b,k} \\right) p\\left(\\zeta_{g,i,d_b,k}\\vert \\mathbf{w}_{\\cdot,d_b,k} \\right) \\right]^{z_{g,k}}, $$</p> <p>where \\(\\boldsymbol{\\zeta}_{g,i,d_b,k}\\) are Polya-gamma densities \\(\\text{PG}(1,\\mathbf{w}^{T}_{\\cdot,d_b,k}\\mathbf{x}_{g,i,\\cdot}) \\) as described in Durante (2019), except in our case we consider a nonparametric mixture of \\(D_b\\) conditionally independent target variables. Importantly, the augmented likelihood is within the exponential family of distributions, and the prior over \\(\\mathbf{W}_b\\) is now conjugate. </p> <p>Durante and Rigon (2019) provide coordinate ascent variational inference updates for the variational distributions \\(p^*\\left(\\mathbf{W}_b\\right) \\) and \\(p^*\\left(\\boldsymbol{\\zeta} \\right)\\) (which in turn relate directly the the EM algorithm proposed by Jaakkola and Jordan (2000)). Extending these updates to our model gives the variational distribution over \\(\\mathbf{W}_b\\) as: $$     p^*(\\mathbf{W}_b) = \\prod_{d_b=1}^{D_b}\\prod_{k=1}^{K}     N(\\mathbf{w}_{\\cdot,d_b,k} \\vert      \\boldsymbol{\\mu}_{d_b,k},\\boldsymbol{\\lambda}^{-1}_{d_b,k}) $$ where $$ \\boldsymbol{\\lambda}^{-1}_{d_b,k} = \\left(\\boldsymbol{\\Sigma}^{-1}_{d_b} + \\mathbf{X}^{T}\\mathbf{G}_{k}\\mathbf{X} \\right)^{-1} $$ and</p> <p>$$     \\mathbf{G}_{k} = \\text{diag}\\{     0.5\\left[\\xi_{1,d_b,k} \\right]^{-1}\\text{tanh}\\left(0.5 \\xi_{1,d_b,k}\\right)r_{1,k}, \\cdots,     0.5\\left[\\xi_{N,d_b,k} \\right]^{-1}\\text{tanh}\\left(0.5 \\xi_{N,d_b,k}\\right)r_{N,k}     \\} $$</p> <p>and</p> <p>$$     \\boldsymbol{\\mu}_{d_b,k}=\\boldsymbol{\\lambda}^{-1}_{d_b,k}\\left[      \\mathbf{X}^{T}\\text{diag}\\{r_{\\cdot,k}\\}     \\left( \\mathbf{y}_{\\cdot,d_b}-0.5\\mathbf{1}_{N} \\right) +     \\mathbf{\\Sigma}_{d_b}\\boldsymbol{\\mu}_{d_b}     \\right] $$</p> <p>Note that in order to more easily express the variational distribution parameters, we have introduced the index \\(n\\) to refer to individual data points: \\((g=1, i=1) \\mapsto n=1, (g=1, i=2) \\mapsto n=2, \\cdots, (g=G, i=n_g) \\mapsto n=N\\). The vector \\( \\boldsymbol{\\mu}_{d_b} \\) is the \\(M\\)-dimensional vector of mean prior values for the \\(d_{b}^{th}\\) dimension for the prior over \\( \\mathbf{W}_b \\). Similarly, \\(\\mathbf{\\Sigma}_{d_b}\\) is the \\(M\\times M\\) diagonal matrix of variance (inverse precision) values for the \\(d_{b}^{th}\\) dimension. \\(r_{n,k}\\) is the probability that data instance \\(n\\) belongs to component \\(k\\) (see above).</p> <p>The variational distribution \\(p^*\\left( \\zeta_{n,d_b,k} \\right)\\) is a Polya-gamma distribution, \\(\\text{PG}(1,\\xi_{n,d_b,k})\\) with $$     \\xi_{n,d_b,k} = \\left[      \\mathbf{x}^{T}_{n,\\cdot}\\boldsymbol{\\lambda}^{-1}_{d_b,k}     \\mathbf{x}_{n,\\cdot} +     \\left(\\mathbf{x}^{T}_{n,\\cdot}\\boldsymbol{\\mu}_{d_b,k}\\right)^{2}     \\right]^{\\frac{1}{2}}. $$</p>"},{"location":"inference/#optimization","title":"Optimization","text":"<p>With these terms define, inference proceeds by iteratively updating the parameters for the variational distributions \\( p^*(\\mathbf{W}_c) \\), \\( p^*(\\mathbf{W}_b) \\), \\( p^{*}(\\mathbf{U}) \\), \\( p^{*}(\\boldsymbol{\\lambda}) \\), \\( p^{*}(\\mathbf{v}) \\), and \\( p^{*}(\\mathbf{Z}) \\) for a prespecified number of iterations, set to ensure successive iterations produce a negligible change in the variational parameters.</p>"}]}