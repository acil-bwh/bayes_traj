
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://github.com/acil-bwh/bayes_traj/inference/">
      
      
        <link rel="prev" href="../formulation/">
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.20">
    
    
      
        <title>Inference - bayes_traj Documentation</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.66ac8b77.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#variational-inference" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="bayes_traj Documentation" class="md-header__button md-logo" aria-label="bayes_traj Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            bayes_traj Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Inference
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="bayes_traj Documentation" class="md-nav__button md-logo" aria-label="bayes_traj Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    bayes_traj Documentation
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../bayes_traj_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tutorial
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../formulation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Formulation
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Inference
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Inference
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    <span class="md-ellipsis">
      Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#variational-distributions" class="md-nav__link">
    <span class="md-ellipsis">
      Variational Distributions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Variational Distributions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#variational-distribution-pmathbfw_c" class="md-nav__link">
    <span class="md-ellipsis">
      Variational distribution \( p^*(\mathbf{W}_c) \)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variational-distribution-pmathbfu" class="md-nav__link">
    <span class="md-ellipsis">
      Variational distribution \( p^{*}(\mathbf{U}) \)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variational-distribution-pboldsymbollambda" class="md-nav__link">
    <span class="md-ellipsis">
      Variational distribution \( p^{*}(\boldsymbol{\lambda}) \)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variational-distribution-pmathbfv" class="md-nav__link">
    <span class="md-ellipsis">
      Variational distribution \( p^{*}(\mathbf{v}) \)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variational-distribution-pmathbfz" class="md-nav__link">
    <span class="md-ellipsis">
      Variational distribution \( p^{*}(\mathbf{Z}) \)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-strategy-for-mathbfw_b" class="md-nav__link">
    <span class="md-ellipsis">
      Update Strategy for \( \mathbf{W}_b \)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Optimization
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    <span class="md-ellipsis">
      Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#variational-distributions" class="md-nav__link">
    <span class="md-ellipsis">
      Variational Distributions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Variational Distributions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#variational-distribution-pmathbfw_c" class="md-nav__link">
    <span class="md-ellipsis">
      Variational distribution \( p^*(\mathbf{W}_c) \)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variational-distribution-pmathbfu" class="md-nav__link">
    <span class="md-ellipsis">
      Variational distribution \( p^{*}(\mathbf{U}) \)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variational-distribution-pboldsymbollambda" class="md-nav__link">
    <span class="md-ellipsis">
      Variational distribution \( p^{*}(\boldsymbol{\lambda}) \)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variational-distribution-pmathbfv" class="md-nav__link">
    <span class="md-ellipsis">
      Variational distribution \( p^{*}(\mathbf{v}) \)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variational-distribution-pmathbfz" class="md-nav__link">
    <span class="md-ellipsis">
      Variational distribution \( p^{*}(\mathbf{Z}) \)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-strategy-for-mathbfw_b" class="md-nav__link">
    <span class="md-ellipsis">
      Update Strategy for \( \mathbf{W}_b \)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Optimization
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="variational-inference">Variational Inference</h1>
<h2 id="overview">Overview</h2>
<p>Frequently with complex models, obtaining the exact posterior distribution can
be intractable. While Markov Chain Monte Carlo (MCMC) 
methods offer a systematic approach to sample from the posterior distribution,
they can be slow in high-dimensional parameter spaces
(<a href="https://www.jstor.org/stable/pdf/2334940.pdf?casa_token=ZEcTsBtCLZkAAAAA:-f5zD4VUydXnjBQe5ErtLRajoF6QScZsOTatDPDjQiawsV_HAjdiNI6T4HmdnmuEbXq8oWXF6DXvQUKrZbNCgc3laLqrW0NWEtyhSS3LMmZa96Odlw">Hastings (1970)</a>).
An alternative is variational inference, a form
of Bayesian approximate inference that tends to be fast and scales well to large
data sets (<a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=642dba1a71033b9587e4cbcb993a8016f012dc00">Jordan (1999)</a>).
It typically makes a factorization assumption over the approximate
posterior distribution of interest, and it turns an inference problem into an
optimization problem by finding the
approximate posterior that minimizes the Kullback-Leibler divergence to the true
posterior distribution 
(see <a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=642dba1a71033b9587e4cbcb993a8016f012dc00">Jordan (1999)</a>,
<a href="http://people.csail.mit.edu/people/tommi/papers/Jaa-nips00-tutorial.pdf">Jaakkola (2001)</a>, and
<a href="http://www.cs.columbia.edu/~blei/fogm/2018F/materials/BleiKucukelbirMcAuliffe2017.pdf">Blei (2017)</a>).</p>
<p>In the case of conjugate priors (a prior is conjugate when the posterior
distribution belongs to the same family of probability distributions as the
prior distribution given a specific likelihood function), there is a straightforward
procedure for deriving variational update eqautions. In the case of
non-conjugate priors, alternative approaches are needed.</p>
<p>For our model, the posterior distribution we wish to estimate is given by:
$$
p\left(\mathbf{W}_c,
\mathbf{W}_b,
\boldsymbol{\lambda},
\mathbf{Z},
\mathbf{v},
\mathbf{U}
\mid
\mathbf{Y}_c,
\mathbf{Y}_b,
\mathbf{X},
\boldsymbol{\mu}_{0_c},
\boldsymbol{\lambda}_{0_c},
\boldsymbol{\mu}_{0_b},
\boldsymbol{\lambda}_{0_b},
\mathbf{a}_0,
\mathbf{b}_0,
\alpha,
\boldsymbol{\Sigma}_0
\right)
$$
This posterior probability is approximated using variational inference. The
standard mean field variational inference approach is to assume a factorized
approximation of this distribution, in our case:
$$
p^*(\mathbf{W}_{c})
p^*(\mathbf{W}_{b})
p^*(\boldsymbol{\lambda})
p^*(\mathbf{Z})
p^*(\mathbf{v})
p^*(\mathbf{U})
$$
In order to derive the expression for each of these factors, the expectation
with respect to the other factors is considered. Derivation of the variational
distributions begins with the following expressions:
$$
\text{ln}p^*(\mathbf{W}_c) =
\mathbb{E}_{\mathbf{W}_b,
\mathbf{\lambda},
\mathbf{Z},
\mathbf{v},
\mathbf{U}
}\{\text{ln}p(\mathbf{Y}_c,
\mathbf{Y}_b,
\mathbf{W}_c,
\mathbf{W}_b,
\boldsymbol{\lambda},
\mathbf{Z},
\mathbf{v},
\mathbf{U}
) \} +
\text{const}
$$</p>
<p>$$
\text{ln}p^*(\mathbf{U}) =
\mathbb{E}_{
\mathbf{W}_c,
\mathbf{W}_b,
\mathbf{\lambda},
\mathbf{Z},
\mathbf{v}
}\{\text{ln}p(\mathbf{Y}_c,
\mathbf{Y}_b,
\mathbf{W}_c,
\mathbf{W}_b,
\boldsymbol{\lambda},
\mathbf{Z},
\mathbf{v},
\mathbf{U}
) \} +
\text{const}
$$</p>
<p>$$
\text{ln}p^*(\boldsymbol{\lambda}) =
\mathbb{E}_{
\mathbf{W}_c,
\mathbf{W}_b,
\mathbf{Z},
\mathbf{v},
\mathbf{U}
}\{ \text{ln}p(\mathbf{Y}_c,
\mathbf{Y}_b,
\mathbf{W}_c,
\mathbf{W}_b,
\boldsymbol{\lambda},
\mathbf{Z},
\mathbf{v},
\mathbf{U}
)\} +
\text{const}
$$</p>
<p>$$
\text{ln}p^*(\mathbf{v}) =
\mathbb{E}_{
\mathbf{W}_c,
\mathbf{W}_b,
\boldsymbol{\lambda},
\mathbf{Z},
\mathbf{U}
}
\{\text{ln}p(\mathbf{Y}_c,
\mathbf{Y}_b,
\mathbf{W}_c,
\mathbf{W}_b,
\mathbf{\lambda},
\mathbf{Z},
\mathbf{v},
\mathbf{U}
) \} + \text{const}
$$</p>
<p>$$
\text{ln}p^*(\mathbf{Z}) =
\mathbb{E}_{
\mathbf{W}_c,
\mathbf{W}_b,
\boldsymbol{\lambda},
\mathbf{v},
\mathbf{U}
}
\{\text{ln}p(\mathbf{Y}_c,
\mathbf{Y}_b,
\mathbf{W}_c,
\mathbf{W}_b,
\boldsymbol{\lambda},
\mathbf{Z},
\mathbf{v},
\mathbf{U}
)\}
+ \text{const}
$$</p>
<p>A challenge arises, however, if priors in the model are not conditionally
conjugate â€“ i.e. if factor posteriors are not in the same family as the
corresponding priors. This is the case with the Gaussian priors for the
coefficients of logistic regression \(\mathbf{W}_b\), meaning that the
distribution \(p^*(\mathbf{W}_b) \) can not be assumed Gaussian unless
approximations are made to restore conjugacy. We address this challenge
by integrating the coordinate ascent variational inference algorithm with
the EM (Expectation-Maximization) algorithm to facilitate updates of
\( p^*(\mathbf{W}_b) \) (see below).</p>
<h2 id="variational-distributions">Variational Distributions</h2>
<p>Here we provide expressions for each of the variational distributions.</p>
<h3 id="variational-distribution-pmathbfw_c">Variational distribution \( p^*(\mathbf{W}_c) \)</h3>
<p>The variational distribution over the coefficients \( \mathbf{W}_c \) is given
by a multivariate Gaussian distribution that factorizes over the predictors,
targets, and subtype clusters:</p>
<p>$$
p^*(\mathbf{W}_c)=
\prod_{m=1}^{M}
\prod_{d_c=1}^{D_c}
\prod_{k=1}^{K}
\mathcal{N}
\left(
w_{m, d_c, k} \mid
\mu_{m, d_c, k},
{\lambda^{-1}_{m, d_c, k}}
\right)
$$</p>
<p>$$
\lambda_{m, d_c, k}=
\lambda_{0_{c_{m, d_c}}}+
\frac{a_{d_c, k}}{b_{d_c, k}}
\sum_{g=1}^{G}
\mathbb{E}_{\mathbf{z}}\{z_{g, k}\}
\sum_{i=1}^{n_g}
x_{i,m}^2
$$</p>
<p>$$
\mu_{m, d_c, k}=
{\lambda_{m, d_c, k}}^{-1}
\bigg[
\mu_{0_{c_{m, d_c}}} {\lambda_{0_{c_{m, d_c}}}} -
\frac{{a_{d_c, k}}}{{b_{d_c, k}}}
\sum_{g=1}^{G}
\mathbb{E}_{\mathbf{z}}\{z_{g, k}\} \times
$$
$$
\sum_{i=1}^{n_g}x_{g,i,m}
\left(\mathbb{E}_{\mathbf{w}}\{\mathbf{w}_{-, d_c, k}\}^{T} \
\mathbf{x}_{g,i,-} +
\mathbb{E}\{ \mathbf{u}_{g,d_c,k} \}^T \mathbf{x}_{g,i,.} -
y_{g,i, d_c}\right)
\bigg] 
\label{Wc_ast}
$$</p>
<p>The \(-\) in \( \mathbf{w}_{-,d_c,k} \) and \( \mathbf{x}_{g,i,-} \)
indicates all but the \(m^{th}\) predictor.</p>
<h3 id="variational-distribution-pmathbfu">Variational distribution \( p^{*}(\mathbf{U}) \)</h3>
<p>The variational distribution for \( p^{*}(\mathbf{U}) \) is given by</p>
<p>$$
p^{*}(\mathbf{U}) =
\prod_{g=1}^{G}
\prod_{d_c=1}^{D_c}
\prod_{k=1}^{K}
\mathcal{N}
\left(
\mathbf{u}_{g, d_c, k} \mid
\boldsymbol{\mu}_{g, d_c, k},
\boldsymbol{\Sigma}_{g, d_c, k}
\right)
$$
where
$$
\boldsymbol{\Sigma}_{g, d_c, k} =
\left[ \boldsymbol{\Sigma}_0^{-1} + 
\mathbb{E}\{ z_{g,k}\}
\frac{a_{d_c,k}}{b_{d_c,k}}
\sum_{i=1}^{n_g}
\mathbf{x}_{g,i,d_c}\mathbf{x}_{g,i,d_c}^T
\right]^{-1}
$$
and
$$
\boldsymbol{\mu}_{g, d_c, k} =
\left[
\mathbb{E}\{ z_{g,k}\}
\frac{a_{d_c,k}}{b_{d_c,k}}
\sum_{i=1}^{n_g}
\left(
\mathbb{E}\{  w_{., d, k}  \}^T
\mathbf{x}_{g,i,d_c} - y_{g, i, d_c}
\right)\mathbf{x}_{g,i,d_c}^T
\right]
\boldsymbol{\Sigma}_{g, d_c, k}
$$</p>
<h3 id="variational-distribution-pboldsymbollambda">Variational distribution \( p^{*}(\boldsymbol{\lambda}) \)</h3>
<p>The variational distribution over \( \mathbf{\lambda} \) is given by a gamma
distribution with parameters \( \mathbf{a} \) and \( \mathbf{b} \):
$$
p^{*}(\boldsymbol{\lambda})=
\prod_{d_c=1}^{D_c}
\prod_{k=1}^{K}
\operatorname{Gam}
\left(\lambda_{d_c, k} \mid a_{d_c, k}, b_{d_c, k}
\right)
$$</p>
<p>$$
a_{d_c, k}=
a_{0_{d_c}}+
\frac{1}{2}
\sum_{g=1}^{G}n_g
\mathbb{E}_{\mathbf{z}}\{z_{g, k}\} 
$$</p>
<p>$$
b_{d_c, k}=
b_{0_{d_c}}+
\frac{1}{2}
\sum_{g=1}^{G}
\sum_{i=1}^{n_g}
\mathbb{E}\{z_{g, k}\}
\bigg(\mathbb{E}\{
\left({\mathbf{w}^{T}_{\cdot, d_c, k}} \mathbf{x}_{g,i, \cdot}
\right)^{2}\}
-2 y_{g,i,d_c}
\mathbb{E}\{\mathbf{w}_{\cdot,d_c, k}\}^{T} \mathbf{x}_{g,i,\cdot} +
{y^{2}_{g,i, d_c}}
$$
$$
-2y_{g,i,d_c}\mathbb{E}\{ \mathbf{u}_{g,d_c,k}\}^T\mathbf{x}_{g,i,.} +
2\mathbb{E}\{ \mathbf{w}_{.,d_c,k}\}^T\mathbf{x}_{g,i,.}
\mathbb{E}\{ \mathbf{u}_{g,d_c,k}\}^T\mathbf{x}_{g,i,.} +
\mathbb{E}\{ ( \mathbf{u}_{g,d_c,k}^T\mathbf{x}_{g,i,.} )^2 \}
\bigg)
\label{lambda_ast}
$$</p>
<h3 id="variational-distribution-pmathbfv">Variational distribution \( p^{*}(\mathbf{v}) \)</h3>
<p>The variational distribution for \( p^{*}(\mathbf{v}) \) is given by
$$
p^{*}(\mathbf{v})=
\prod_{k=1}^{K}
\text{Beta}
\left( 
v_k \mid 1 +
\sum_{g=1}^{G}
\mathbb{E}\{\mathbf{Z} \}_{g, k}, 
\alpha +
\sum_{j=k+1}^{K}
\sum_{g=1}^{G}
\mathbb{E}\{\mathbf{Z} \}_{g, j}
\right)
\label{v_ast}
$$
where \( K \) is an integer (e.g. 20) chosen by the user for the truncated stick-breaking process.</p>
<h3 id="variational-distribution-pmathbfz">Variational distribution \( p^{*}(\mathbf{Z}) \)</h3>
<p>The variational distribution for \( p^{*}(\mathbf{Z}) \) is given by</p>
<p>$$
p^{*}(\mathbf{Z})=
\prod_{g=1}^{G}\prod_{k=1}^{K}r_{g,k}^{z_{g,k}}
\label{pZstar}
$$</p>
<p>where</p>
<p>$$
r_{g,k}=\frac{\rho_{g,k}}{\sum_{k=1}^{K}\rho_{g,k}} \label{eq:rnk}
$$</p>
<p>and</p>
<p>$$
\ln \rho_{g, k} = n_g\mathbb{E}\{\ln v_k\} +
n_g\sum_{j=1}^{k-1}\mathbb{E}\{\ln (1-v_j)\} + \
$$</p>
<p>$$
\frac{1}{2}\sum_{d_c=1}^{D_c}
\bigg[n_g\mathbb{E}\{\ln \lambda_{d_c, k}\}-n_g\ln (2 \pi) -
$$
$$
\mathbb{E}\{\lambda_{d_c, k}\}
\sum_{i=1}^{n_g}
\bigg(\mathbb{E}\{\left({\mathbf{w}^{T}_{\cdot, d_c, k}} \mathbf{x}_{g,i,\cdot}\right)^2\} -
2 y_{g,i,d_c} \mathbb{E}\{{\mathbf{w}_{\cdot, d_c, k}}\}^{T} \mathbf{x}_{g,i,\cdot}+y^2_{g,i, d_c} +
$$</p>
<p>$$
-2y_{g,i,d_c}\mathbb{E}\{ \mathbf{u}_{g,d_c,k}\}^T\mathbf{x}_{g,i,.} +
2\mathbb{E}\{ \mathbf{w}_{.,d_c,k}\}^T\mathbf{x}_{g,i,.}
\mathbb{E}\{ \mathbf{u}_{g,d_c,k}\}^T\mathbf{x}_{g,i,.} +
\mathbb{E}\{ ( \mathbf{u}_{g,d_c,k}^T\mathbf{x}_{g,i,.} )^2 \}
\bigg)\bigg] +
$$
$$
\sum_{d_b=1}^{D_b}
\sum_{i=1}^{n_g}
\left[
y_{g,i, d_b} \mathbb{E}\{
    {\mathbf{w}_{\cdot, d_b, k}}\}^{T}{\mathbf{x}_{g,i, \cdot}}
    -\mathbb{E}\{\ln\left(1+\exp\left(\mathbf{x}_{g,i, \cdot}\cdot{\mathbf{w}_{\cdot, d_b, k}}^T\right)\right)\}
    \right]
$$</p>
<h2 id="update-strategy-for-mathbfw_b">Update Strategy for \( \mathbf{W}_b \)</h2>
<p>As mentioned above, the Gaussian priors over the coefficients
\( \mathbf{W}_b \) are not conjugate concerning the likelihood factor
\( p(\mathbf{Y}_b\mid \mathbf{Z}, \mathbf{W}_b) \).
When the prior and likelihood are not conjugate, Bayesian inference becomes more
complex and computationally demanding since the posterior distribution cannot be
derived analytically. 
Our methodology applies a tangent quadratic lower bound to the logistic
likelihoods within the framework of variational inference for conditionally
conjugate exponential family models 
(see <a href="https://www.jstor.org/stable/pdf/26874191.pdf?acceptTC=true&amp;coverpage=false&amp;addFooter=false&amp;casa_token=5tSjE7iYI8wAAAAA:9UYtJvE_cGh2930jPcoCnSuWlK7-scKaCRwS1LwsRrF2_Uwq5qGsA-PcU4P_QAlJWwI8-M86kfGTfdCwFjYGZ39HW1SGCbGA_RYGk9iDog3yZgYJxQ">Durante (2019)</a>).
This approach restores conjugacy between the approximate bounds and the
Gaussian priors on \( \mathbf{W}_b \).</p>
<p><a href="http://www2.stat.duke.edu/homeweb/scs/Courses/Stat376/Papers/Variational/JaakkolaJordan2000.pdf">Jaakkola and Jordan (2000)</a>
introduced a straightforward variational approach based
on a family of tangent quadratic lower bounds of logistic log-likelihoods. They
derived an EM algorithm to iteratively refine the variational parameters of the
lower bound and the mean and covariance of the Gaussian distribution over the
predictor coefficients. However, this method was specifically designed for
simple logistic regression and did not extend to mixtures of logistic
regressors. To address this, we extend these concepts to Dirichlet Process
mixture models in our formulation. We can augment the likelihood function
\( p(\mathbf{Y}_b\mid \mathbf{Z}, \mathbf{W}_b) \):</p>
<p>$$
p(\mathbf{Y}_b, \boldsymbol{\zeta} \mid \mathbf{Z}, \mathbf{W}_b)=
\prod_{k=1}^{\infty}\prod_{g=1}^{G} \prod_{d_b=1}^{D_b} 
\left[\prod_{i=1}^{n_g}
p\left( y_{g,i,d_b}\vert \mathbf{w}_{\cdot,d_b,k} \right)
p\left(\zeta_{g,i,d_b,k}\vert \mathbf{w}_{\cdot,d_b,k} \right)
\right]^{z_{g,k}},
$$</p>
<p>where \(\boldsymbol{\zeta}_{g,i,d_b,k}\) are Polya-gamma densities
\(\text{PG}(1,\mathbf{w}^{T}_{\cdot,d_b,k}\mathbf{x}_{g,i,\cdot}) \)
as described in
<a href="https://www.jstor.org/stable/pdf/26874191.pdf?acceptTC=true&amp;coverpage=false&amp;addFooter=false&amp;casa_token=5tSjE7iYI8wAAAAA:9UYtJvE_cGh2930jPcoCnSuWlK7-scKaCRwS1LwsRrF2_Uwq5qGsA-PcU4P_QAlJWwI8-M86kfGTfdCwFjYGZ39HW1SGCbGA_RYGk9iDog3yZgYJxQ">Durante (2019)</a>,
except in our case we consider a nonparametric mixture of \(D_b\)
conditionally independent target variables. Importantly, the augmented
likelihood is within the exponential family of distributions, and the prior
over \(\mathbf{W}_b\) is now conjugate. </p>
<p><a href="https://www.jstor.org/stable/pdf/26874191.pdf?acceptTC=true&amp;coverpage=false&amp;addFooter=false&amp;casa_token=5tSjE7iYI8wAAAAA:9UYtJvE_cGh2930jPcoCnSuWlK7-scKaCRwS1LwsRrF2_Uwq5qGsA-PcU4P_QAlJWwI8-M86kfGTfdCwFjYGZ39HW1SGCbGA_RYGk9iDog3yZgYJxQ">Durante and Rigon (2019)</a>
provide coordinate ascent variational inference
updates for the variational distributions \(p^*\left(\mathbf{W}_b\right) \)
and \(p^*\left(\boldsymbol{\zeta} \right)\) (which in turn relate directly
the the EM algorithm proposed by
<a href="http://www2.stat.duke.edu/homeweb/scs/Courses/Stat376/Papers/Variational/JaakkolaJordan2000.pdf">Jaakkola and Jordan (2000)</a>).
Extending these updates to our model gives the variational distribution over
\(\mathbf{W}_b\) as:
$$
    p^*(\mathbf{W}_b) = \prod_{d_b=1}^{D_b}\prod_{k=1}^{K}
    N(\mathbf{w}_{\cdot,d_b,k} \vert 
    \boldsymbol{\mu}_{d_b,k},\boldsymbol{\lambda}^{-1}_{d_b,k})
$$
where
$$
\boldsymbol{\lambda}^{-1}_{d_b,k} = \left(\boldsymbol{\Sigma}^{-1}_{d_b} +
\mathbf{X}^{T}\mathbf{G}_{k}\mathbf{X} \right)^{-1}
$$
and</p>
<p>$$
    \mathbf{G}_{k} = \text{diag}\{<br />
    0.5\left[\xi_{1,d_b,k} \right]^{-1}\text{tanh}\left(0.5 \xi_{1,d_b,k}\right)r_{1,k}, \cdots,
    0.5\left[\xi_{N,d_b,k} \right]^{-1}\text{tanh}\left(0.5 \xi_{N,d_b,k}\right)r_{N,k}
    \}
$$</p>
<p>and</p>
<p>$$
    \boldsymbol{\mu}_{d_b,k}=\boldsymbol{\lambda}^{-1}_{d_b,k}\left[ 
    \mathbf{X}^{T}\text{diag}\{r_{\cdot,k}\}
    \left( \mathbf{y}_{\cdot,d_b}-0.5\mathbf{1}_{N} \right) +
    \mathbf{\Sigma}_{d_b}\boldsymbol{\mu}_{d_b}
    \right]
$$</p>
<p>Note that in order to more easily express the variational distribution
parameters, we have introduced the index \(n\) to refer to individual data
points: \((g=1, i=1) \mapsto n=1, (g=1, i=2) \mapsto n=2, \cdots,
(g=G, i=n_g) \mapsto n=N\). The vector \( \boldsymbol{\mu}_{d_b} \) is
the \(M\)-dimensional vector of mean prior values for the \(d_{b}^{th}\)
dimension for the prior over \( \mathbf{W}_b \). Similarly,
\(\mathbf{\Sigma}_{d_b}\) is the \(M\times M\) diagonal matrix of variance
(inverse precision) values for
the \(d_{b}^{th}\) dimension. \(r_{n,k}\) is
the probability that data instance \(n\) belongs to component \(k\)
(see above).</p>
<p>The variational distribution \(p^*\left( \zeta_{n,d_b,k} \right)\) is a
Polya-gamma distribution, \(\text{PG}(1,\xi_{n,d_b,k})\) with
$$
    \xi_{n,d_b,k} = \left[ 
    \mathbf{x}^{T}_{n,\cdot}\boldsymbol{\lambda}^{-1}_{d_b,k}
    \mathbf{x}_{n,\cdot} +
    \left(\mathbf{x}^{T}_{n,\cdot}\boldsymbol{\mu}_{d_b,k}\right)^{2}
    \right]^{\frac{1}{2}}.
$$</p>
<h2 id="optimization">Optimization</h2>
<p>With these terms define, inference proceeds by iteratively updating the parameters for
the variational distributions
\( p^*(\mathbf{W}_c) \),
\( p^*(\mathbf{W}_b) \),
\( p^{*}(\mathbf{U}) \),
\( p^{*}(\boldsymbol{\lambda}) \),
\( p^{*}(\mathbf{v}) \), and
\( p^{*}(\mathbf{Z}) \) for a prespecified number of
iterations, set to ensure successive iterations produce a negligible change in the
variational parameters.</p>







  
    
  
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1-2.1-2M12.5 7v5.2l4 2.4-1 1L11 13V7h1.5M11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2v1.8Z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">September 13, 2024</span>
  </span>

    
    
    
    
  </aside>





                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.instant", "search.highlight"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.dd8806f2.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src=".."></script>
      
        <script src=".."></script>
      
    
  </body>
</html>