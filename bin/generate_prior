#!/usr/bin/env python

from argparse import ArgumentParser
import pickle
import pandas as pd
import statsmodels.api as sm
import numpy as np
import pdb
from provenance_tools.write_provenance_data import write_provenance_data

def prior_info_from_model(target_name, mm, prior_info):
    """This function estimates prior values from draws of the specified model
    posterior. Estimates are made for a specific target variable. The assumption
    is the model's predictors and the predictor set for which estimates are
    desired are the same.

    Parameters
    ----------
    target_name : str
        Name of the target variable for which predictor prior values will be 
        estimated.

    mm : MultDPRegression instance
        Trajectory model containing a fit to data. The trajectories in this 
        model will be used to estimate the prior values.

    prior_info : dict
        Prior data structure that will be updated by this function
    """
    assert set(prior_info['w_mu0'][target_name].keys()) == \
        set(mm.predictor_names_), "Predictor name mismatch"

    traj_ids = np.where(mm.sig_trajs_)[0]
    traj_probs = np.sum(mm.R_, 0)/np.sum(mm.R_)
    
    num_traj_samples = np.random.multinomial(10000, traj_probs)

    model_target_index = \
        np.where(np.array(mm.target_names_, dtype=str) == target_name)[0][0]
    
    for m in range(mm.M_):
        samples = []
        for t in traj_ids:
            samples.append(mm.w_mu_[m, model_target_index, t] + \
                           np.sqrt(mm.w_var_[m, model_target_index, t])*\
                           np.random.randn(num_traj_samples[t]))

            prior_info['w_mu0'][target_name][mm.predictor_names_[m]] = \
                np.mean(np.hstack(samples))
            prior_info['w_var0'][target_name][mm.predictor_names_[m]] = \
                np.var(np.hstack(samples))  

    # For precision parameters, we'll use a similar sample-based procedure as
    # was done for the coefficients
    samples = []
    for t in traj_ids:
        scale_tmp = 1./mm.lambda_b_[model_target_index, t]
        shape_tmp = mm.lambda_a_[model_target_index, t]
        samples.append(np.random.gamma(shape_tmp, scale_tmp,
                                       num_traj_samples[t]))

        prior_info['lambda_a0'][target_name] = \
            np.mean(np.hstack(samples))**2/np.var(np.hstack(samples))
        prior_info['lambda_b0'][target_name] = \
	    np.mean(np.hstack(samples))/np.var(np.hstack(samples))

            
desc = """Generates a pickled file containing Bayesian trajectory prior 
information"""

parser = ArgumentParser(description=desc)
parser.add_argument('--preds', help='Comma-separated list of predictor names',
    dest='preds', type=str, default=None)
parser.add_argument('--targets', help='Comma-separated list of target names',
    dest='targets', type=str, default=None)
parser.add_argument('--out_file', help='Output (pickle) file that will \
    contain the prior', dest='out_file', type=str, default=None)
parser.add_argument('-k', help='Number of columns in the truncated assignment \
    matrix', metavar='<int>', default=20)
parser.add_argument('--tar_resid', help='Residual standard deviation prior \
    value for specified target. Specify as a comma-separated tuple:  \
    target_name,mean,std. Here, mean is the expected value of the residual \
    standard deviation, and std reflects the strength of the prior (lower \
    values indicate more confidence in the mean value)', type=str,
    default=None, action='append', nargs='+')
parser.add_argument('--coef', help='Coefficient prior for a specified \
    target and predictor. Specify as a comma-separated tuple: \
    target_name,predictor_name,mean,std', type=str,
    default=None, action='append', nargs='+')
parser.add_argument('--coef_std', help='Coefficient prior standard deviation \
    for a specified target and predictor. Specify as a comma-separated tuple: \
    target_name,predictor_name,std', type=str, default=None,
    action='append', nargs='+')
parser.add_argument('--in_data', help='If a data file is specified, it will be \
    read in and used to set reasonable prior values using OLS regression. It \
    is assumed that the file contains data columns with names corresponding \
    to the predictor and target names specified on the command line.',
    type=str, default=None)
parser.add_argument('--num_trajs', help='Rough estimate of the number of \
    trajectories expected in the data set.', type=int, default=2)
parser.add_argument('--model', help='Pickled bayes_traj model that \
      has been fit to data and from which information will be extracted to \
      produce an updated prior file', type=str, default=None)

op = parser.parse_args()

preds = op.preds.split(',')
targets = op.targets.split(',')

D = len(targets)
M = len(preds)
K = float(op.k)

#-------------------------------------------------------------------------------
# Initialize prior info
#-------------------------------------------------------------------------------
prior_info = {}

prior_info['w_mu0'] = {}
prior_info['w_var0'] = {}
prior_info['lambda_a0'] = {}
prior_info['lambda_b0'] = {}

for tt in targets:
    prior_info['w_mu0'][tt] = {}
    prior_info['w_var0'][tt] = {}
    prior_info['lambda_a0'][tt] = 1
    prior_info['lambda_b0'][tt] = 1        
    
    for pp in preds:
        prior_info['w_mu0'][tt][pp] = 0
        prior_info['w_var0'][tt][pp] = 5

# Guesstimate of how big a data sample. Will be used to generate an estimate of
# alpha. Will be overwritten if a data file has been specified.
N = 10000
prior_info['alpha'] = op.num_trajs/np.log10(N)

#-------------------------------------------------------------------------------
# If a model has been specified, use it to set prior values
#-------------------------------------------------------------------------------
if op.model is not None:
    with open(op.model, 'rb') as f:
        mm = pickle.load(f)['MultDPRegression']

    if mm.gb_ is not None:
        prior_info['alpha'] = np.sum(mm.sig_trajs_)/np.log10(mm.gb_.ngroups)
    else:
        prior_info['alpha'] = np.sum(mm.sig_trajs_)/np.log10(mm.N_)
        
    if set(mm.predictor_names_) == set(preds):
        for tt in targets:
            if tt in mm.targets_:
                prior_info_from_model(tt, mm, prior_info)
            

#-------------------------------------------------------------------------------
# If a data file has been specified
#-------------------------------------------------------------------------------
if op.in_data is not None:
    df = pd.read_csv(op.in_data)
    N = df.shape[0]
    for tt in targets:
        res_tmp = sm.OLS(df[tt], df[preds], missing='drop').fit()

        gamma_mean = 1./(np.nanvar(res_tmp.resid)/op.num_trajs)
        gamma_var = 1e-5 # Might want to expose this to user
        prior_info['lambda_b0'][tt] = gamma_mean/gamma_var
        prior_info['lambda_a0'][tt] = gamma_mean**2/gamma_var        
        for pp in preds:
            prior_info['w_mu0'][tt][pp] = res_tmp.params[pp]

            # The following defaults result in a reasonable spread of
            # trajectories in synthetic experiments
            tmp = pp.split('^')
            if len(tmp) > 1:
                prior_info['w_var0'][tt][pp] = 10**(-int(tmp[-1])*5)
            else:
                prior_info['w_var0'][tt][pp] = 5e-4
                
# Generate a rough estimate of alpha
prior_info['alpha'] = op.num_trajs/np.log10(N)


#-------------------------------------------------------------------------------
# Override prior settings with user-specified preferences
#-------------------------------------------------------------------------------
if op.tar_resid is not None:
    for i in range(len(op.tar_resid)):
        tt = op.tar_resid[i][0].split(',')[0]
        mean_tmp = float(op.tar_resid[i][0].split(',')[1])
        std_tmp = float(op.tar_resid[i][0].split(',')[2])        
        assert tt in targets, "{} not among specified targets".format(tt)
                
        gamma_mean = 1./(mean_tmp**2)
        gamma_var = std_tmp**2
        prior_info['lambda_b0'][tt] = gamma_mean/gamma_var
        prior_info['lambda_a0'][tt] = gamma_mean**2/gamma_var

if op.coef is not None:
    for i in range(len(op.coef)):
        tt = op.coef[i][0].split(',')[0]
        pp = op.coef[i][0].split(',')[1]
        m = float(op.coef[i][0].split(',')[2])
        s = float(op.coef[i][0].split(',')[3])

        assert tt in targets, "{} not among specified targets".format(tt)
        assert pp in preds, "{} not among specified predictors".format(pp)        

        prior_info['w_mu0'][tt][pp] = m
        prior_info['w_var0'][tt][pp] = s**2

if op.coef_std is not None:
    for i in range(len(op.coef_std)):
        tt = op.coef_std[i][0].split(',')[0]
        pp = op.coef_std[i][0].split(',')[1]
        s = float(op.coef_std[i][0].split(',')[2])

        assert tt in targets, "{} not among specified targets".format(tt)
        assert pp in preds, "{} not among specified predictors".format(pp)        

        prior_info['w_var0'][tt][pp] = s**2

#-------------------------------------------------------------------------------
# Summarize prior info and save to file
#-------------------------------------------------------------------------------        
print('---------- Prior Info ----------')
print('alpha: {}'.format(prior_info['alpha']))        
for tt in targets:
    tar_mean = prior_info['lambda_b0'][tt]/prior_info['lambda_a0'][tt]
    tar_std = np.sqrt(prior_info['lambda_a0'][tt]/\
                      (prior_info['lambda_b0'][tt]**2))
    print("{} residual (mean, std): ({}, {})".format(tt, tar_mean, tar_std))
    for pp in preds:
        tmp_mean = prior_info['w_mu0'][tt][pp]
        tmp_std = np.sqrt(prior_info['w_var0'][tt][pp])
        print("{} {} (mean, std): ({}, {})".format(tt, pp, tmp_mean, tmp_std))
        
if op.out_file is not None:                    
    pickle.dump(prior_info, open(op.out_file, 'wb'))
    desc = """ """
    write_provenance_data(op.out_file, generator_args=op, desc=desc,
                          module_name='bayes_traj')
