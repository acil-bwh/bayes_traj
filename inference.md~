
# Dirichlet Process Mixtures
Ferguson [@Ferguson1973] first introduced the Dirichlet process (DP) as a
measure on measures. It is parameterized by a base measure,
\\(\mathit{G}\_0\\), and a positive scaling parameter \\( \alpha \\):
$$
  \mathit{G} \vert \\{ \mathit{G}\_0, \alpha \\} \sim \text{DP}\left( \mathit{G}\_0, \alpha \right)
$$
The notion of a Dirichlet process mixture (DPM) arises if we treat the
\\( k^{th} \\) draw from \\( \mathit{G} \\) as a parameter of the distribution over some
observation [@Antoniak1974]. DPMs can be interpreted as mixture models with an
infinite number of mixture components.

More recently, Blei and Jordan [@Blei2006] described a variational inference
algorithm for DPMs using the stick-breaking construction introduced in
[@Sethuraman1991]. The stick-breaking construction represents \\( G \\) as
$$
  \pi\_{k}( \mathbf{v} ) = \mathbf{v}\_{k}\prod^{k-1}_{j=1}\left( 1-\mathbf{v}\_{j}\right)
$$

$$
\mathit{G} = \sum\_{i=1}^{\infty}\pi\_{i}\left( \mathbf{v}\right)\delta\_{\eta\_{i}^{\ast}} 
$$
where \\( \delta\_{\eta\_{i}^{\ast}}   \\) is the Kronecker delta, and the
\\( \mathbf{v}\_{i} \\) are distributed according to a beta distribution:
\\( \mathbf{v}\_{i} \sim \text{Beta}\left( 1, \alpha \right) \\), and 
\\( \eta_{i}^{\ast} \sim \mathit{G}\_0 \\). We use a DPM in our model to
automatically identify the number of disease trajectories that best explain
our data.


# Model Formulation
Here we provide our Bayesian nonparametric model formulation. 
We consider a collection of multiple longitudinally observed target variables,
which can be continuous, binary, or a combination.
We let \\(y_{g,i,d}\\) represent the observation for individual \\(g\\)
\\( (g=1,\dots,G) \\) at the \\( i^{th} \\) occasion \\( (i=1,\dots,n_g) \\) for
target variable \\(d\\). 
Similarly, \\(x_{g,i,m}\\) represents predictor \\(m\\) \\( (m=1,\dots,M)\\) for
individual \\(g\\) on occasion \\(i\\).
Here, \\(G\\) is the total number of individuals in the data sample, \\(n_g\\)
is the number of observations per individual, and \\(M\\) is the number of
predictors. The likelihood in our formulation factorizes into two terms:
$$
p(\mathbf{Y}\mid \mathbf{Z}, \mathbf{W}, \boldsymbol{\lambda}) =
p(\mathbf{Y}_c\mid \mathbf{Z}, \mathbf{W}_c, \boldsymbol{\lambda})
p(\mathbf{Y}_b\mid \mathbf{Z}, \mathbf{W}_b)
$$
where we distinguish between the collection of \\(D_c\\) continuous target
variables, \\(Y_c\\), and the collection of \\(D_b\\) binary target variables,
\\(Y_b\\). The likelihood factors are given by:

$$
p(\mathbf{Y}_c\mid \mathbf{Z}, \mathbf{W}_c, \boldsymbol{\lambda})=\prod\_{k=1}^{\infty} \prod\_{g=1}^{G}\prod\_{d_c=1}^{D_c}\left[\prod\_{i=1}^{n_g}\mathcal{N}\left(y\_{g,i,d_c} \mid \mathbf{w}\_{\cdot, d_c, k}^{T} \mathbf{x}\_{g,i,\cdot}, \lambda\_{d_c,k}^{-1}\right)\right]^{z\_{g, k}}
$$
and
$$
p(\mathbf{Y}_b\mid \mathbf{Z}, \mathbf{W}_b)=\prod\_{k=1}^{\infty}\prod\_{g=1}^{G} \prod\_{d_b=1}^{D_b} 
\left[\prod\_{i=1}^{n_g}
\frac{\exp(\mathbf{w}\_{\cdot, d_b, k}^{T} \mathbf{x}\_{g,i,\cdot})^{y\_{g,i,d_b}}}
{1+\exp(\mathbf{w}\_{\cdot, d_b, k}^{T} \mathbf{x}\_{g,i,\cdot})}
\right]^{z\_{g,k}}.
\label{YbLike}
$$

We formulate our model as a DPMM, which can be interpreted as a mixture model
with a potentially infinite number of mixture components ([@antoniak1974mixtures]).
The \\(G \times \infty\\) binary indicator matrix, \\(\mathbf{Z}\\), represents
the association between subjects and the potentially infinite number of latent
regression functions, and \\(k\\) represents the group membership for each individual.
In the case of \\(\mathbf{Y}_c\\) this formulation can be see as mixture of linear
regressors, and in the case of \\(\mathbf{Y}_b\\) it can be seen as an infinite
mixture of logistic regressors.  

\\( \mathbf{W}\_c \\) represents the \\(M \times D_c \times \infty\\) matrix of
predictor coefficients for the linear regressors, and  \\( \mathbf{W}\_b \\)
represents the \\(M \times D_b \times \infty\\) matrix of predictor coefficients for
the logistic regressors. We put Gaussian priors over both \\( \mathbf{W}\_c\\) and
\\(\mathbf{W}\_b\\):

$$
p\left(\mathbf{W}\_c \mid \boldsymbol{\mu}\_c, \boldsymbol{\lambda}\_c\right)=\prod\_{m=1}^{M} \prod\_{d_c=1}^{D_c} \prod\_{k=1}^{\infty} \mathcal{N}\left(w\_{m, d_c, k} \mid \mu\_{c_{m, d_c}}, \lambda\_{c_{m, d_c}}^{-1}\right)
$$

and
$$
p\left(\mathbf{W}\_b \mid \boldsymbol{\mu}\_b, \boldsymbol{\lambda}\_b\right)=\prod_\{m=1}^{M} \prod_\{d_b=1}^{D_b} \prod_\{k=1}^{\infty} \mathcal{N}\left(w_{m, d_b, k} \mid \mu_\{b_{m, d_b}}, \lambda_\{b_{m, d_b}}^{-1}\right)
\label{Wbprior}
$$

We learn the residual precisions, \\( \boldsymbol{\lambda} \\), for each of the
\\(D_c \times \infty\\) linear regressors, and place gamma priors over these terms:
$$
p(\boldsymbol{\lambda} \mid \mathbf{a}\_0, \mathbf{b}\_0)=\prod\_{k=1}^{\infty} \prod\_{d_c=1}^{D_c} \mathrm{Gam}\left(\lambda\_{d_c, k} \mid a\_{0_{d_c}}, b\_{0_{d_c}}\right).
$$

The nonparametric prior distribution over \\( \mathbf{Z} \\) is given by:
$$
p(\mathbf{Z} \mid \mathbf{v})=
\prod\_{g=1}^{G} \prod\_{k=1}^{\infty}\left(v_{k} \prod\_{j=1}^{k-1}\left(1-v_{j}\right)\right)^{z_{g, k}}
$$
This can be considered a \\( G\times \infty \\) multinomial distribution with
parameters drawn for a DP using the stick-breaking construction
[@blei2006variational,@sethuraman1994constructive], where the elements
of \\(\mathbf{v}\\) are drawn from a beta distribution with concentration parameter \\(\alpha\\):
$$
p(\mathbf{v} \mid \alpha)=\prod\_{k=1}^{\infty} \mathrm{Beta}\left(v_{k} \mid 1, \alpha\right).
\label{prior_v}
$$
The concentration parameter \\(\alpha\\) captures the practitioner’s prior
belief about whether there are fewer groups (low scale parameter value) or
more groups (larger scale parameter value).
A benefit of the non-parametric framework is that the number of components
that best describe the observed data is automatically determined conditioned on
this value.

With these terms defined, the joint density is given as
$$
p\left(\mathbf{Y}\_c, 
\mathbf{Y}\_b, 
\mathbf{W}\_c, 
\mathbf{W}\_b, 
\boldsymbol{\lambda}, 
\mathbf{Z}, 
\mathbf{v}, 
\mid 
\mathbf{X},
\boldsymbol{\mu}\_{c}, 
\boldsymbol{\lambda}\_c, 
\boldsymbol{\mu}\_b, 
\boldsymbol{\lambda}\_b, 
\mathbf{a}\_0, 
\mathbf{b}\_0, 
\alpha, \right) = \\\
p(\mathbf{Y}\_c \mid \mathbf{Z}, \mathbf{W}\_c, \boldsymbol{\lambda})
p(\mathbf{Y}\_b \mid \mathbf{Z}, \mathbf{W}\_b)
p\left(\mathbf{W}\_c \mid \boldsymbol{\mu}\_c, \boldsymbol{\lambda}\_c\right)
p\left(\mathbf{W}\_b \mid \boldsymbol{\mu}\_b, \boldsymbol{\lambda}\_{b}\right) \\\
p(\boldsymbol{\lambda} \mid \mathbf{a}\_0, \mathbf{b}\_0)
p(\mathbf{Z} \mid \mathbf{v}) 
p(\mathbf{v} \mid \alpha)
$$

# Variational Inference
Frequently with complex models, obtaining the exact posterior distribution can
be intractable. While Markov Chain Monte Carlo (MCMC) 
methods offer a systematic approach to sample from the posterior distribution,
they can be slow in high-dimensional parameter spaces [@hastings1970monte]. 
An alternative is variational inference [@jordan1999introduction], a form
of Bayesian approximate inference that tends to be fast and scales well to large
data sets. It typically makes a factorization assumption over the approximate
posterior distribution of interest, and it turns an inference problem into an
optimization problem [@jordan1999introduction, @jaakkola200110] by finding the
approximate posterior that minimizes the Kullback-Leibler divergence to the true
posterior distribution [@blei2017variational].

In the case of conjugate priors (a prior is conjugate when the posterior
distribution belongs to the same family of probability distributions as the
prior distribution given a specific likelihood function), there is a straightforward
procedure for deriving variational update eqautions. In the case of
non-conjugate priors, alternative approaches are needed.


complexities from
non-conjugate priors for coefficients estimation in infinite DP mixture of logistic regression. 

For our model, the posterior distribution we wish to estimate is given by:
$$
p\left(\mathbf{W}\_c, \mathbf{W}\_b, \boldsymbol{\lambda}, \mathbf{Z}, \mathbf{v}, \mid \mathbf{Y}\_c, \mathbf{Y}\_b, \mathbf{X},\boldsymbol{\mu}\_{c}, \boldsymbol{\lambda}\_c, \boldsymbol{\mu}\_b, \boldsymbol{\lambda}\_b, \mathbf{a}\_0, \mathbf{b}\_0, \alpha, \right)
$$
This posterior probability is approximated using variational inference. The
standard mean field variational inference approach is to assume a factorized
approximation of this distribution, in our case:
$$
p^\*(\mathbf{W}\_{c}) p^\*(\mathbf{W}\_{b}) p^\*(\boldsymbol{\lambda}) p^\*(\mathbf{Z}) p^\*(\mathbf{v})
$$
In order to derive the expression for each of these factors, the expectation
with respect to the other factors is considered. Derivation of the variational
distributions begins with the following expressions:
$$
\text{ln}p^\*(\mathbf{Z}) = \mathbb{E}\_{\mathbf{W}\_c, \mathbf{W}\_b,\boldsymbol{\lambda},\mathbf{v}}
\\{\text{ln}p(\mathbf{Y}\_c, \mathbf{Y}\_b, \mathbf{W}\_c, \mathbf{W}\_b, \boldsymbol{\lambda}, \mathbf{Z}, \mathbf{v})\\}
+ \text{const}
$$

$$
\text{ln}p^\*(\mathbf{v}) = \mathbb{E}\_{\mathbf{W}\_c,\mathbf{W}\_b,\boldsymbol{\lambda},\mathbf{Z}}\\{\text{ln}p(\mathbf{Y}\_c, \mathbf{Y}\_b, \mathbf{W}\_c, \mathbf{W}\_b, \mathbf{\lambda}, \mathbf{Z}, \mathbf{v}) \\} + \text{const}
$$

$$
\text{ln}p^\*(\boldsymbol{\lambda}) = \mathbb{E}\_{\mathbf{W}_c, \mathbf{W}\_b,\mathbf{Z},\mathbf{v}}\\{ \text{ln}p(\mathbf{Y}\_c, \mathbf{Y}\_b, \mathbf{W}\_c, \mathbf{W}\_b, \boldsymbol{\lambda}, \mathbf{Z}, \mathbf{v})\\} + \text{const}
$$

$$
\text{ln}p^\*(\mathbf{W}\_c) = \mathbb{E}\_{\mathbf{W}\_b, \mathbf{\lambda}, \mathbf{Z}, \mathbf{v}}\\{\text{ln}p(\mathbf{Y}\_c, \mathbf{Y}\_b, \mathbf{W}_c, \mathbf{W}\_b, \boldsymbol{\lambda}, \mathbf{Z}, \mathbf{v}) \\} + \text{const}
$$

A challenge arises, however, if priors in the model are not conditionally
conjugate – i.e. if factor posteriors are not in the same family as the
corresponding priors. This is the case with the Gaussian priors for the
coefficients of logistic regression \\(\mathbf{W}\_b\\), meaning that the
distribution \\(p^\*(\mathbf{W}\_b) \\) can not be assumed Gaussian unless
approximations are made to restore conjugacy. We address this challenge
by integrating the coordinate ascent variational inference algorithm with
the EM (Expectation-Maximization) algorithm to facilitate updates of
\\( p^\*(\mathbf{W}\_b) \\).


## Update Strategy for \\( \mathbf{W}\_b \\) 
As mentioned above, the Gaussian priors over the coefficients
\\( \mathbf{W}\_b \\) are not conjugate concerning the likelihood factor
\\( p(\mathbf{Y}_b\mid \mathbf{Z}, \mathbf{W}_b) \\).
When the prior and likelihood are not conjugate, Bayesian inference becomes more
complex and computationally demanding since the posterior distribution cannot be
derived analytically. Our methodology lies in enabling variational updates of the
posterior distribution for \\( \mathbf{W}_b \\) through the use of a tangent
quadratic lower bound of logistic likelihoods within the framework of variational
inference for conditionally conjugate exponential family models
[@durante2019conditionally] for an infinite mixture of logistic regression
under DPMMs to restore conjugacy between these approximate bounds and the
Gaussian priors on \\( \mathbf{W}_b \\). 

[@jaakkola2000bayesian] introduced a straightforward variational approach based
on a family of tangent quadratic lower bounds of logistic log-likelihoods. They
derived an EM algorithm to iteratively refine the variational parameters of the
lower bound and the mean and covariance of the Gaussian distribution over the
predictor coefficients. However, this method was specifically designed for
simple logistic regression and did not extend to mixtures of logistic
regressors. To address this, we extend these concepts to Dirichlet Process
mixture models in our formulation. We can augment the likelihood function
\\( p(\mathbf{Y}_b\mid \mathbf{Z}, \mathbf{W}_b) \\):

$$
p(\mathbf{Y}\_b, \boldsymbol{\zeta} \mid \mathbf{Z}, \mathbf{W}\_b)=
\prod_\{k=1}^{\infty}\prod\_{g=1}^{G} \prod\_{d_b=1}^{D_b} 
\left[\prod\_{i=1}^{n\_g}
p\left( y\_{g,i,d_b}\vert \mathbf{w}\_{\cdot,d_b,k} \right)
p\left(\zeta\_{g,i,d_b,k}\vert \mathbf{w}\_{\cdot,d_b,k} \right)
\right]^{z_{g,k}},
$$

where \\(\boldsymbol{\zeta}\_{g,i,d_b,k}\\) are Polya-gamma densities
\\(\text{PG}(1,\mathbf{w}^{T}\_{\cdot,d_b,k}\mathbf{x}\_{g,i,\cdot}) \\)
as described in [@durante2019conditionally], except in our case we consider
a nonparametric mixture of \\(D\_b\\) conditionally independent target
variables. Importantly, the augmented likelihood is within the exponential
family of distributions, and the prior over \\(\mathbf{W}\_b\\)
is now conjugate. 

[@durante2019conditionally] provide coordinate ascent variational inference
updates for the variational distributions \\(p^\*\left(\mathbf{W}\_b\right) \\)
and \\(p^\*\left(\boldsymbol{\zeta} \right)\\) (which in turn relate directly
the the EM algorithm proposed by [@jaakkola2000bayesian]). Extending these
updates to our model gives the variational distribution over
\\(\mathbf{W}\_b\\) as
$$
    p^\*(\mathbf{W}\_b) = \prod\_{d_b=1}^{D_b}\prod_{k=1}^{K}
    N(\mathbf{w}\_{\cdot,d_b,k} \vert 
    \boldsymbol{\mu}\_{d_b,k},\boldsymbol{\lambda}^{-1}\_{d_b,k})
$$
where
$$
\boldsymbol{\lambda}^{-1}\_{d_b,k} = \left(\boldsymbol{\Sigma}^{-1}\_{d_b} +
\mathbf{X}^{T}\mathbf{G}\_{k}\mathbf{X} \right)^{-1}
$$
and

$$
    \mathbf{G}\_{k} = \text{diag}\\{  
    0.5\left[\xi\_{1,d_b,k} \right]^{-1}\text{tanh}\left(0.5 \xi\_{1,d_b,k}\right)r\_{1,k}, \cdots,
    0.5\left[\xi\_{N,d_b,k} \right]^{-1}\text{tanh}\left(0.5 \xi\_{N,d_b,k}\right)r\_{N,k}
    \\}
$$

and

$$
    \boldsymbol{\mu}\_{d_b,k}=\boldsymbol{\lambda}^{-1}\_{d_b,k}\left[ 
    \mathbf{X}^{T}\text{diag}\\{r\_{\cdot,k}\\}
    \left( \mathbf{y}\_{\cdot,d_b}-0.5\mathbf{1}\_{N} \right) +
    \mathbf{\Sigma}\_{d_b}\boldsymbol{\mu}\_{d_b}
    \right]
$$

Note that in order to more easily express the variational distribution
parameters, we have introduced the index \\(n\\) to refer to individual data
points: \\((g=1, i=1) \mapsto n=1, (g=1, i=2) \mapsto n=2, \cdots,
(g=G, i=n_g) \mapsto n=N\\). The vector \\( \boldsymbol{\mu}\_{d_b} \\) is
the \\(M\\)-dimensional vector of mean prior values for the \\(d\_{b}^{th}\\)
dimension in Equation \ref{Wbprior}. Similarly, \\(\mathbf{\Sigma}\_{d_b}\\) is
the \\(M\times M\\) diagonal matrix of variance (inverse precision) values for
the \\(d\_{b}^{th}\\) dimension in Equation~\ref{Wbprior}. \\(r\_{n,k}\\) is
the probability that data instance \\(n\\) belongs to component \\(k\\) as
given in Equation \ref{rnk}.

The variational distribution \\(p^\*\left( \zeta\_{n,d_b,k} \right)\\) is a
Polya-gamma distribution, \\(\text{PG}(1,\xi\_{n,d_b,k})\\) with
$$
    \xi\_{n,d_b,k} = \left[ 
    \mathbf{x}^{T}\_{n,\cdot}\boldsymbol{\lambda}^{-1}\_{d_b,k}
    \mathbf{x}\_{n,\cdot} +
    \left(\mathbf{x}^{T}\_{n,\cdot}\boldsymbol{\mu}_{d_b,k}\right)^{2}
    \right]^{\frac{1}{2}}.
$$

Inference proceeds by iteratively updating the parameters for the variational
distributions \\(p^\*\left( \mathbf{v}\right)\\),
\\(p^{*}\left(\mathbf{Z} \right)\\), \\(p^\*\left(\mathbf{W}\_c \right)\\),
\\(p^\*\left( \boldsymbol{\lambda} \right)\\), \\(p^\*\left( \mathbf{W}_b \right)\\),
and \\p^\*\left(\boldsymbol{\zeta} \right)\\) for a prespecified number of
iterations or until successive iterations produce a negligible change in the
variational parameters.

<!--

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\cite{durante2019conditionally} provide coordinate ascent variational inference updates for the variational distributions $p^{*}\left( \mathbf{W}_b \right)$ and $p^{*}\left(\boldsymbol{\zeta} \right)$ (which in turn relate directly the the EM algorithm proposed by \citep{jaakkola2000bayesian}). Extending these updates to our model gives the variational distribution over $\mathbf{W}_b$ as
\begin{gather}
    p^{*}(\mathbf{W}_b) = \prod_{d_b=1}^{D_b}\prod_{k=1}^{K}N(\mathbf{w}_{\cdot,d_b,k} \vert 
    \boldsymbol{\mu}_{d_b,k},\boldsymbol{\lambda}^{-1}_{d_b,k}),
    \label{Wb_ast}
\end{gather}
\noindent where
\begin{gather}
    \boldsymbol{\lambda}^{-1}_{d_b,k} = \left(\boldsymbol{\Sigma}^{-1}_{d_b} + \mathbf{X}^{T}\mathbf{G}_{k}\mathbf{X} \right)^{-1}
\end{gather}
\noindent and
\begin{gather}
    \mathbf{G}_{k} = \text{diag}\left\{  
    0.5\left[\xi_{1,d_b,k} \right]^{-1}\text{tanh}\left(0.5 \xi_{1,d_b,k}\right)r_{1,k}, \cdots,
    0.5\left[\xi_{N,d_b,k} \right]^{-1}\text{tanh}\left(0.5 \xi_{N,d_b,k}\right)r_{N,k}
    \right\}
\end{gather}
\noindent and
\begin{gather}
    \boldsymbol{\mu}_{d_b,k}=\boldsymbol{\lambda}^{-1}_{d_b,k}\left[ 
    \mathbf{X}^{T}\text{diag}\left\{r_{\cdot,k}\right\}\left( \mathbf{y}_{\cdot,d_b}-0.5\matbf{1}_{N} \right) + \mathbf{\Sigma}_{d_b}\boldsymbol{\mu}_{d_b}
    \right].
\end{gather}

\noindent Note that in order to more easily express the variational distribution parameters, we have introduced the index $n$ to refer to individual data points: $(g=1, i=1) \mapsto n=1, (g=1, i=2) \mapsto n=2, \cdots, (g=G, i=n_g) \mapsto n=N$. The vector $\boldsymbol{\mu}_{d_b}$ is the $M$-dimensional vector of mean prior values for the $d_{b}^{th}$ dimension in Equation \ref{Wbprior}. Similarly, $\mathbf{\Sigma}_{d_b}$ is the $M\times M$ diagonal matrix of variance (inverse precision) values for the $d_{b}^{th}$ dimension in \ref{Wbprior}. $r_{n,k}$ is the probability that data instance $n$ belongs to component $k$ as given in Equation \ref{rnk}.

The variational distribution $p^{*}\left( \zeta_{n,d_b,k} \right)$ is a P\'{o}lya-gamma distribution, $\text{PG}(1,\xi_{n,d_b,k})$ with
\begin{gather}
    \xi_{n,d_b,k} = \left[ 
    \mathbf{x}^{T}_{n,\cdot}\boldsymbol{\lambda}^{-1}_{d_b,k}\mathbf{x}_{n,\cdot} + \left( \mathbf{x}^{T}_{n,\cdot}\boldsymbol{\mu}_{d_b,k}  \right)^{2}
    \right]^{\frac{1}{2}}
\end{gather}

Inference proceeds by iteratively updating the parameters for the variational distributions $p^{*}\left( \mathbf{v}\right)$, $p^{*}\left(\mathbf{Z} \right)$, $p^{*}\left(\mathbf{W}_c \right)$, $p^{*}\left( \boldsymbol{\lambda} \right)$, $p^{*}\left( \mathbf{W}_b \right)$, and $p^{*}\left(\boldsymbol{\zeta} \right)$ for a prespecified number of iterations or until successive iterations produce a negligible change in the variational parameters.


The variational distribution for $p^{*}(\mathbf{v})$ is given by
\begin{gather}
p^{*}(\mathbf{v})=\prod_{k=1}^{K}\text{Beta}
\left( 
v_k \mid 1 + \sum_{g=1}^{G}\mathbb{E}_{\mathbf{Z}}\{\mathbf{Z} \}_{g, k}, 
\alpha + \sum_{j=k+1}^{K}\sum_{g=1}^{G}\mathbb{E}_{\mathbf{Z}}\{\mathbf{Z} \}_{g, j}
\right)
\label{v_ast}
\end{gather}
\noindent where $K$ is an integer chosen by the user for the truncated stick-breaking process (set to 20 in our experiments).

The variational distribution for $p^{*}(\mathbf{Z})$ is given by
\begin{gather}
p^{*}(\mathbf{Z})=
\prod_{g=1}^{G}\prod_{k=1}^{K}r_{g,k}^{z_{g,k}}
\label{pZstar}
\end{gather}
\noindent where
\begin{gather}
r_{g,k}=\frac{\rho_{g,k}}{\sum_{k=1}^{K}\rho_{g,k}}
\label{rnk}
\end{gather}
\noindent and 
\begin{gather}
\ln \rho_{g, k} = n_g\mathbb{E}_{\mathbf{v}}\{\ln v_k\} + n_g\sum_{j=1}^{k-1}\mathbb{E}_\mathbf{v}\{\ln (1-v_j)\} + \notag \\
    \frac{1}{2} 
    \sum_{d_c=1}^{D_c} \bigg[n_g\mathbb{E}_{\boldsymbol{\lambda}}\left\{\ln \lambda_{d_c, k}\right\}-n_g\ln (2 \pi)- \notag \\ 
    \mathbb{E}_{\boldsymbol{\lambda}}\left\{\lambda_{d_c, k}\right\}\sum_{i=1}^{n_g}\bigg(\mathbb{E}_{\mathbf{W}_c}\left\{\left({\mathbf{w}^{T}_{\cdot, d_c, k}} \mathbf{x}_{g,i,\cdot}\right)^{2}\right\}%\notag\\
    -2 y_{g,i,d_c} \mathbb{E}_{\mathbf{W}_c}\left\{{\mathbf{w}_{\cdot, d_c, k}}\right\}^{T} \mathbf{x}_{g,i,\cdot}+y^2_{g,i, d_c}\bigg)\bigg] + \notag \\
    \sum_{d_b=1}^{D_b}\sum_{i=1}^{n_g}\left[
    y_{g,i, d_b} \mathbb{E}_{\mathbf{W}_b}\left\{
    {\mathbf{w}_{\cdot, d_b, k}}\right\}^{T}{\mathbf{x}_{g,i, \cdot}}
    -\mathbb{E}_{\mathbf{W}_b}\left\{\ln\left(1+\exp\left(\mathbf{x}_{g,i, \cdot}\cdot{\mathbf{w}_{\cdot, d_b, k}}^T\right)\right)\right\}
    \right]
\end{gather}

%&\psZ = \pVV \Bigg[ \frac{1}{\Zpf_{\tV}}  \exp \lp -\sidnv \Hin \rp \nonumber \\
%&\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \prod_{\itn \in \text{V}} \pKk r_{\itn,\itk}^{\Z_{\itn,\itk}} \Bigg] \label{eq:z_ast}
%\end{align}
%where %\vspace{-3mm}
%\begin{align}
%  r_{n,k}=\frac{\rho_{n,k}}{\sIk \rho_{n,k}}
%\end{align}
%\noindent and %\vspace{-3mm}
%\begin{gather}
%  \ln \rho_{n,k}= \Ev\left\{ \ln v_{k}\right\}  + \skj \Ev\left\{ \ln \left( 1-v_{j}\right) \right\} + \nonumber \\
%  \frac{1}{2}\sDd \left[ \E\{ \text{ln}\boldsymbol\lambda_{d,k}\}  -\text{ln}(2\pi)
%    - \nonumber \\
%    \E\{\boldsymbol\lambda_{d,k}\} \left(  \E\bigg\{\left(\W_{\cdot,d,k}^T\X_{n,\cdot}\right)^2 \bigg\}  - \nonumber \\
%      2\Y_{n,d}\E\{\W_{\cdot,d,k} \}^T\X_{n,\cdot}  + \Y_{n,d}^2\bigg) \bigg] 
%\end{gather}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The variational distribution over the coefficients $\mathbf{W}_c$ is given by a multivariate Gaussian distribution that factorizes over the predictors, targets, and subtype clusters:
\begin{gather}
p^{*}(\mathbf{W}_c)=\prod_{m=1}^{M} \prod_{d_c=1}^{D_c} \prod_{k=1}^{K} \mathcal{N}\left(w_{m, d_c, k} \mid \mu_{m, d_c, k}, {\lambda^{-1}_{m, d_c, k}}\right) \\
\lambda_{m, d_c, k}=\lambda_{c_{m, d_c}}+\frac{a_{d_c, k}}{b_{d_c, k}} \sum_{g=1}^{G} \mathbb{E}_{\mathbf{z}}\left\{z_{g, k}\right\}\sum_{i=1}^{n_g} {x^{2}_{g,i, m}} \\
\mu_{m, d_c, k}= {\lambda_{m, d_c, k}}^{-1}\left[\mu_{c_{m, d_c}} {\lambda_{c_{m, d_c}}} -\frac{{a_{d_c, k}}}{{b_{d_c, k}}} \sum_{g=1}^{G} \mathbb{E}_{\mathbf{z}}\left\{z_{g, k}\right\} 
\sum_{i=1}^{n_g}x_{g,i, m}\left(\mathbb{E}_{\mathbf{w}}\left\{\mathbf{w}_{-, d_c, k}\right\}^{T} \mathbf{x}_{g,i,-}-y_{g,i, d_c}\right)\right] 
\label{Wc_ast}
\end{gather}
\noindent The $-$ in $\mathbf{w}_{-,d,k}$ and $\mathbf{x}_{n,-}$ indicates all but the $m^{th}$ predictor.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The variational distribution over $\mathbf{\lambda}$ is given by a gamma distribution with parameters $\mathbf{a}$ and $\mathbf{b}$:
\begin{gather}
p^{*}(\mathbf{\lambda})=\prod_{d_c=1}^{D_c} \prod_{k=1}^{K} \operatorname{Gam}\left(\lambda_{d_c, k} \mid a_{d_c, k}, b_{d_c, k}\right) \\
a_{d_c, k}=a_{0_{d_c}}+\frac{1}{2} \sum_{g=1}^{G}n_g \mathbb{E}_{\mathbf{z}}\left\{z_{g, k}\right\} \\
b_{d_c, k}=b_{0_{d_c}}+\frac{1}{2} \sum_{g=1}^{G}\sum_{i=1}^{n_g} \mathbb{E}_{\mathbf{z}}\left\{z_{g, k}\right\}\left(\mathbb{E}_{\mathbf{w}}\left\{\left({\mathbf{w}^{T}_{\cdot, d_c, k}} \mathbf{x}_{g,i, \cdot}\right)^{2}\right\}\right.
\left.-2 y_{g,i,d_c} \mathbb{E}\left\{\mathbf{w}_{\cdot,d_c, k}\right\}^{T} \mathbf{x}_{g,i,,\cdot}+{y^{2}_{g,i, d_c}}\right)
\label{lambda_ast}
\end{gather}














###### <TMI>
Let \\( \mathbf{X} = \left[ \mathbf{x}_{1} \cdots \mathbf{x}_M     \right] \\)
be the \\( N \times M \\) matrix of observed predictors where \\(N\\) is the number of
instances and \\(M\\) is the dimension of the predictor space.
Let \\( \mathbf{Y} = \left[ \mathbf{y}_1 \cdots \mathbf{y}_D \right] \\)
be the \\(N \times D\\) matrix of corresponding target values, where \\(D\\) represents	
the dimension of the target variables. We introduce the \\(N \times \infty\\) binary
indicator matrix, \\( \mathbf{Z} \\), to represent the association between the
data instances and the latent regression functions.
\\( \mathbf{W} \\) is the \\(M \times D \times \infty\\) matrix of predictor
coefficients that we wish to learn,
along with \\( \boldsymbol{\lambda} \\), the \\(D \times \infty\\) matrix of
precision values on each of \\(D\\) target variables.


| **Variable** | **Description** |
| --- | --- |
| \( N \) | Number of data instances |
| \( D \) | Dimension of target variables |
| \( M \) | Dimension of predictor space |
| \( \mathbf{Y} \) | \( N \times D \) matrix of target variables |
| \( \mathbf{X} \) | \( N \times M \) matrix of observed predictors |
| \( \mathbf{W} \) | \( M \times D \times \infty \) matrix of predictor coefficients |
| \( \mathbf{Z} \) | \( N \times \infty \) binary indicator matrix |
| \( \boldsymbol{\lambda} \) | \( D \times \infty \) matrix of precision values |
| \( \mathcal{C} \) | Set of longitudinal constraints |
| \( \mathbf{v} \) | Beta-distributed random variable in stick-breaking construction |
| \( \alpha \) | Hyperparameter for prior over \( \mathbf{v} \) |
| \( \boldsymbol{\mu_0} \), \( \boldsymbol{\lambda_0} \) | Hyperparameters for prior over \( \mathbf{W} \) |
| \( \mathbf{a_0} \), \( \mathbf{b_0} \) | Hyperparameters for prior over \( \boldsymbol{\lambda} \) |

*Table 1: Description of variables in our probabilistic model. See text for details.*

######</TMI>
-->